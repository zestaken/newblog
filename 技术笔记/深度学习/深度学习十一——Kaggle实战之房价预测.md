---
title: æ·±åº¦å­¦ä¹ åä¸€â€”â€”Kaggleå®æˆ˜ä¹‹æˆ¿ä»·é¢„æµ‹
date: 2021-11-26 12:02:19
tags: [æ·±åº¦å­¦ä¹ ]
categories: æŠ€æœ¯ç¬”è®°
---

# Kaggleæˆ¿ä»·é¢„æµ‹æ¯”èµ›ç®€ä»‹

* [æ¯”èµ›åœ°å€](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
* ç«èµ›æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚æ¯æ¡è®°å½•éƒ½åŒ…æ‹¬æˆ¿å±‹çš„å±æ€§å€¼å’Œå±æ€§ï¼Œå¦‚è¡—é“ç±»å‹ã€æ–½å·¥å¹´ä»½ã€å±‹é¡¶ç±»å‹ã€åœ°ä¸‹å®¤çŠ¶å†µç­‰ã€‚è¿™äº›ç‰¹å¾ç”±å„ç§æ•°æ®ç±»å‹ç»„æˆã€‚ä¾‹å¦‚ï¼Œå»ºç­‘å¹´ä»½ç”±æ•´æ•°è¡¨ç¤ºï¼Œå±‹é¡¶ç±»å‹ç”±ç¦»æ•£ç±»åˆ«è¡¨ç¤ºï¼Œå…¶ä»–ç‰¹å¾ç”±æµ®ç‚¹æ•°è¡¨ç¤ºã€‚è¿™å°±æ˜¯ç°å®è®©äº‹æƒ…å˜å¾—å¤æ‚çš„åœ°æ–¹ï¼šä¾‹å¦‚ï¼Œä¸€äº›æ•°æ®å®Œå…¨ä¸¢å¤±äº†ï¼Œç¼ºå¤±å€¼è¢«ç®€å•åœ°æ ‡è®°ä¸ºâ€œNAâ€ã€‚æ¯å¥—æˆ¿å­çš„ä»·æ ¼åªå‡ºç°åœ¨è®­ç»ƒé›†ä¸­ï¼ˆæ¯•ç«Ÿè¿™æ˜¯ä¸€åœºæ¯”èµ›ï¼‰ã€‚
* è®­ç»ƒæ¨¡å‹æ—¶åˆ’åˆ†è®­ç»ƒé›†ä»¥åˆ›å»ºéªŒè¯é›†ï¼Œæ¥åˆ¤æ–­æ¨¡å‹çš„æ•ˆæœã€‚
* æœ€ç»ˆå°†å°†é¢„æµ‹ç»“æœä¸Šä¼ åˆ°Kaggleä¹‹åï¼Œåœ¨å®˜æ–¹æµ‹è¯•é›†ä¸­è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ã€‚

# æ•°æ®è·å–

* å…ˆä»kaggleæ¯”èµ›ç½‘å€ä¸Šå°†æ•°æ®é›†ä¸‹è½½ä¸‹æ¥ã€‚
* å†å°†æ•°æ®å¯¼å…¥åˆ°ç¨‹åºå¯¹è±¡ä¸­ä»¥ä¾›ä½¿ç”¨ã€‚


```python
import pandas as pd
import numpy as np
import tensorflow as tf
```

* è®­ç»ƒæ•°æ®é›†åŒ…æ‹¬1460ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬80ä¸ªç‰¹å¾å’Œ1ä¸ªæ ‡ç­¾ï¼Œè€Œæµ‹è¯•æ•°æ®åŒ…å«1459ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬80ä¸ªç‰¹å¾ã€‚


```python
train_data = pd.read_csv("./house-prices/train.csv")
test_data = pd.read_csv("./house-prices/test.csv")
train_data.shape,test_data.shape
```




    ((1460, 81), (1459, 80))



# æ•°æ®é¢„å¤„ç†

* concatå°†è®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†çºµå‘è¿æ¥èµ·æ¥[concatç”¨æ³•](https://zhuanlan.zhihu.com/p/69224745)
* å°†è®­ç»ƒé›†çš„idåˆ—ï¼ˆç¬¬0åˆ—ï¼‰å’Œæ ‡ç­¾åˆ—ï¼ˆæœ€åä¸€åˆ—ï¼Œå³æˆ¿ä»·åˆ—ï¼‰å»é™¤å†å’Œå»é™¤idåˆ—çš„testé›†åˆå¹¶
* [æ£€æŸ¥NaNå€¼çš„æ–¹æ³•](https://www.cnblogs.com/songdanzju/p/7497566.html)


```python
all_features = pd.concat((train_data.iloc[:,1:-1], test_data.iloc[:,1:])) # å°†æµ‹è¯•é›†å’Œè®­ç»ƒé›†æ‰€æœ‰ç‰¹å¾å€¼åˆå¹¶
all_features.shape
```




    (2919, 79)



* å°†æ‰€æœ‰ç¼ºå¤±çš„å€¼ï¼ˆNaNå€¼ï¼‰æ›¿æ¢ä¸ºç›¸åº”ç‰¹å¾çš„å¹³å‡å€¼ã€‚
* ä¸ºäº†å°†æ‰€æœ‰ç‰¹å¾æ”¾åœ¨ä¸€ä¸ªå…±åŒçš„å°ºåº¦ä¸Šï¼Œæˆ‘ä»¬(**é€šè¿‡å°†ç‰¹å¾é‡æ–°ç¼©æ”¾åˆ°é›¶å‡å€¼å’Œå•ä½æ–¹å·®æ¥æ ‡å‡†åŒ–æ•°æ®**)ï¼š
    $$x \leftarrow \frac{x - \mu}{\sigma}.$$


```python
numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index # è·å–å€¼ä¸ºæ•°å€¼çš„ç‰¹å¾çš„ç´¢å¼•
# æˆ‘ä»¬é€šè¿‡å°†ç‰¹å¾é‡æ–°ç¼©æ”¾åˆ°é›¶å‡å€¼å’Œå•ä½æ–¹å·®æ¥æ ‡å‡†åŒ–æ•°æ®(å°†æ‰€æœ‰çš„æ•°å€¼æ•°æ®ç¼©è‡³åŒä¸€èŒƒå›´)
all_features[numeric_features] = all_features[numeric_features].apply(lambda x : (x - x.mean()) / (x.std()))
all_features[numeric_features] = all_features[numeric_features].fillna(0) # å°†ç¼ºå¤±å€¼å¡«å……ä¸º0
all_features[numeric_features].isnull().any() # æ£€æŸ¥ï¼Œç¡®å®šå·²ç»æ²¡æœ‰æœ‰NaNå€¼çš„åˆ—ï¼ˆæœ‰NaNå€¼åˆ—ä¸ºTrueï¼‰
```




    MSSubClass       False
    LotFrontage      False
    LotArea          False
    OverallQual      False
    OverallCond      False
    YearBuilt        False
    YearRemodAdd     False
    MasVnrArea       False
    BsmtFinSF1       False
    BsmtFinSF2       False
    BsmtUnfSF        False
    TotalBsmtSF      False
    1stFlrSF         False
    2ndFlrSF         False
    LowQualFinSF     False
    GrLivArea        False
    BsmtFullBath     False
    BsmtHalfBath     False
    FullBath         False
    HalfBath         False
    BedroomAbvGr     False
    KitchenAbvGr     False
    TotRmsAbvGrd     False
    Fireplaces       False
    GarageYrBlt      False
    GarageCars       False
    GarageArea       False
    WoodDeckSF       False
    OpenPorchSF      False
    EnclosedPorch    False
    3SsnPorch        False
    ScreenPorch      False
    PoolArea         False
    MiscVal          False
    MoSold           False
    YrSold           False
    dtype: bool



* å°†éæ•°å€¼çš„ç‰¹å¾å€¼è½¬æ¢ä¸ºç‹¬çƒ­ç¼–ç ã€‚å¦‚ï¼Œâ€œMSZoningâ€åŒ…å«å€¼â€œRLâ€å’Œâ€œRmâ€ã€‚å°†åˆ›å»ºä¸¤ä¸ªæ–°çš„æŒ‡ç¤ºå™¨ç‰¹å¾â€œMSZoning_RLâ€å’Œâ€œMSZoning_RMâ€ï¼Œå…¶å€¼ä¸º0æˆ–1ã€‚* pandasè‡ªå¸¦çš„å‡½æ•°å¯ä»¥å®Œæˆè¿™ä¸ªè¿‡ç¨‹ã€‚


```python
# `Dummy_na=True` å°†â€œnaâ€ï¼ˆç¼ºå¤±å€¼ï¼‰è§†ä¸ºæœ‰æ•ˆçš„ç‰¹å¾å€¼ï¼Œå¹¶ä¸ºå…¶åˆ›å»ºæŒ‡ç¤ºç¬¦ç‰¹å¾ã€‚
all_features = pd.get_dummies(all_features, dummy_na=True) # 
all_features.shape # åˆ—æ•°å¢å¤šï¼Œå³å¾ˆå¤šç¦»æ•£çš„éæ•°å€¼ç‰¹å¾å€¼è¢«æ‹†åˆ†æˆç‹¬çƒ­ç¼–ç å¯¼è‡´çš„
```




    (2919, 331)



* é€šè¿‡è¿‡valueså±æ€§ï¼Œæˆ‘ä»¬å¯ä»¥ä»pandasæ ¼å¼ä¸­æå–NumPyæ ¼å¼ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå¼ é‡è¡¨ç¤ºç”¨äºè®­ç»ƒã€‚


```python
n_train = train_data.shape[0] # è®­ç»ƒæ•°æ®çš„æ•°é‡
train_features = tf.constant(all_features[:n_train].values, dtype=tf.float32) # ä»å…¨éƒ¨ç‰¹å¾å€¼ä¸­å–å‡ºè®­ç»ƒç‰¹å¾å€¼
test_features = tf.constant(all_features[n_train : ].values, dtype=tf.float32)# ä»å…¨éƒ¨ç‰¹å¾å€¼ä¸­å–å‡ºæµ‹è¯•ç‰¹å¾å€¼
train_labels = tf.constant(train_data.SalePrice.values.reshape(-1, 1), dtype=tf.float32) # ä»è®­ç»ƒæ•°æ®ä¸­å–å‡ºæ ‡ç­¾åˆ—
```

# è®­ç»ƒ

* æˆ‘ä»¬è®­ç»ƒä¸€ä¸ªå¸¦æœ‰æŸå¤±å¹³æ–¹çš„çº¿æ€§æ¨¡å‹ã€‚
* [L2èŒƒæ•°ç”¨äºæƒé‡è¡°å‡ï¼ˆç®€æ´å®ç°)](https://www.zestaken.top/post/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%83%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E3%80%81%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88)


```python
loss = tf.keras.losses.MeanSquaredError() # å¹³æ–¹æŸå¤±å‡½æ•°
def get_net(): # å®šä¹‰è·å–æ¨¡å‹çš„å‡½æ•°
    net = tf.keras.Sequential() # åºåˆ—æ¨¡å‹
    net.add(tf.keras.layers.Dense( # æ·»åŠ ä¸€ä¸ªå…¨è¿æ¥å±‚
            1, kernel_regularizer = tf.keras.regularizers.l2(weight_decay))) # æ·»åŠ L2èŒƒæ•°æƒ©ç½šé¡¹
    return net
```

* ç”¨ä»·æ ¼é¢„æµ‹çš„å¯¹æ•°æ¥è¡¡é‡å·®å¼‚ã€‚äº‹å®ä¸Šï¼Œè¿™ä¹Ÿæ˜¯æ¯”èµ›ä¸­å®˜æ–¹ç”¨æ¥è¯„ä»·æäº¤è´¨é‡çš„è¯¯å·®æŒ‡æ ‡ã€‚å³å°† $\delta$ for $|\log y - \log \hat{y}| \leq \delta$è½¬æ¢ä¸º$e^{-\delta} \leq \frac{\hat{y}}{y} \leq e^\delta$ã€‚è¿™ä½¿å¾—é¢„æµ‹ä»·æ ¼çš„å¯¹æ•°ä¸çœŸå®æ ‡ç­¾ä»·æ ¼çš„å¯¹æ•°ä¹‹é—´å‡ºç°ä»¥ä¸‹å‡æ–¹æ ¹è¯¯å·®ï¼š
    $$\sqrt{\frac{1}{n}\sum_{i=1}^n\left(\log y_i -\log \hat{y}_i\right)^2}.$$
* å®šä¹‰ä½¿ç”¨å¯¹æ•°è®¡ç®—ç›¸å¯¹è¯¯å·®çš„å‡½æ•°


```python
def log_rmse(y_true, y_pred): # ä¼ å…¥å®é™…æ ‡ç­¾å€¼ï¼Œå’Œé¢„æµ‹æ ‡ç­¾å€¼
     # ä¸ºäº†åœ¨å–å¯¹æ•°æ—¶è¿›ä¸€æ­¥ç¨³å®šè¯¥å€¼ï¼Œå°†å°äº1çš„å€¼è®¾ç½®ä¸º1
    clipped_preds = tf.clip_by_value(y_pred, 1, float('inf')) # å¤„ç†åçš„é¢„æµ‹æ ‡ç­¾å€¼
    return tf.sqrt(tf.reduce_mean(
                    loss(tf.math.log(y_true), tf.math.log(clipped_preds))
                    )) # è®¡ç®—ç›¸å¯¹è¯¯å·®
```

* å®šä¹‰è®­ç»ƒå‡½æ•°
* æ³¨ï¼š[d2lä¸­å‡½æ•°å®šä¹‰](https://www.zestaken.top/post/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0) [Adamä¼˜åŒ–å™¨](https://zh-v2.d2l.ai/chapter_optimization/adam.html)


```python
from d2l import tensorflow as d2l

def train(net, train_features, train_labels, test_features, test_labels,
         num_epochs, learning_rate, weight_decay, batch_size):
    train_ls = []
    test_ls = [] # å­˜å‚¨è®¡ç®—çš„ç›¸å¯¹è¯¯å·®å€¼
    train_iter = d2l.load_array((train_features, train_labels), batch_size) # å°†è®­ç»ƒç‰¹å¾å€¼å’Œè®­ç»ƒæ ‡ç­¾ç”Ÿæˆå°æ‰¹é‡è¿­ä»£å™¨
    # å®šä¹‰Adamä¼˜åŒ–å™¨
    optimizer = tf.keras.optimizers.Adam(learning_rate) 
    # ç”Ÿæˆæ¨¡å‹
    net.compile(loss=loss, optimizer=optimizer)
    # è¿­ä»£è®­ç»ƒ
    for epoch in range(num_epochs):
        for X, y in train_iter: # è¿­ä»£å¾—åˆ°å°æ‰¹é‡ç‰¹å¾å€¼å’Œæ ‡ç­¾
            with tf.GradientTape() as tape:
                y_hat = net(X) # ç”±æ¨¡å‹è·å–é¢„æµ‹å€¼
                l = loss(y, y_hat) # è®¡ç®—æŸå¤±å€¼
            params = net.trainable_variables # è·å–æ¨¡å‹çš„å‚æ•°å€¼
            grads = tape.gradient(l, params) # è®¡ç®—æŸå¤±å€¼å…³äºæ¨¡å‹å‚æ•°å€¼çš„æ¢¯åº¦
            optimizer.apply_gradients(zip(grads, params)) # æ ¹æ®æ¢¯åº¦ä½¿ç”¨ä¼˜åŒ–å™¨ä¼˜åŒ–å‚æ•°
        train_ls.append(log_rmse(train_labels, net(train_features))) # è®¡ç®—æ ‡ç­¾å€¼å’Œè®­ç»ƒåæ¨¡å‹çš„é¢„æµ‹å€¼çš„ç›¸å¯¹è¯¯å·®å¹¶å­˜å‚¨åˆ°æ•°ç»„ä¸­
        if test_labels is not None:
            test_ls.append(log_rmse(test_labels, net(test_features))) # è®¡ç®—æµ‹è¯•æ•°æ®çš„ç›¸å¯¹è¯¯å·®
    return train_ls, test_ls # è¿”å›ç›¸å¯¹è¯¯å·®å€¼æ•°ç»„
```

# KæŠ˜äº¤å‰éªŒè¯ä¼˜åŒ–æ¨¡å‹

* [KæŠ˜äº¤å‰éªŒè¯](https://www.zestaken.top/post/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%83%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E3%80%81%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88)ç”¨æ¥é€‰æ‹©æ¨¡å‹ï¼Œæˆ–è€…é€‰æ‹©åŒä¸€æ¨¡å‹çš„ä¸åŒè¶…å‚æ•°
* æˆ‘ä»¬é¦–å…ˆéœ€è¦ä¸€ä¸ªå‡½æ•°ï¼Œåœ¨ ğ¾ æŠ˜äº¤å‰éªŒè¯è¿‡ç¨‹ä¸­è¿”å›ç¬¬ ğ‘– æŠ˜çš„æ•°æ®ä½œä¸ºéªŒè¯æ•°æ®é›†ï¼Œå‰©ä½™k-1æŠ˜ç»„æˆè®­ç»ƒæ•°æ®é›†ã€‚
* æ³¨ï¼š[sliceå‡½æ•°ä½¿ç”¨](https://www.w3school.com.cn/python/ref_func_slice.asp) [tf.concatä½¿ç”¨](https://blog.csdn.net/leviopku/article/details/82380118)


```python
def get_k_fold_data(k, i, X, y):
        assert k > 1
        fold_size = X.shape[0] // k # å®šä¹‰æ¯ä¸€æŠ˜çš„æ•°æ®é‡ï¼ˆXçš„æ•°æ®é‡é™¤ä»¥æŠ˜æ•°å¹¶å‘ä¸‹å–æ•´ï¼‰
        X_train, y_train = None, None
        for j in range(k):
            idx = slice(j * fold_size, (j + 1) * fold_size) # è·å–ä¸€æŠ˜æ•°é‡çš„ä¸‹æ ‡
            X_part  = X[idx, :] # å–ä¸€æŠ˜æ•°é‡çš„çš„ç‰¹å¾å‘é‡
            y_part = y[idx] # å–å¯¹åº”ä¸€æŠ˜æ•°é‡çš„æ ‡ç­¾é‡
            if j == i: # å–å‡ºç¬¬iæŠ˜çš„å…ƒç´ ä½œä¸ºéªŒè¯æ•°æ®é›†
                X_valid, y_valid = X_part, y_part
            elif X_train is None: # å½“è®­ç»ƒæ•°æ®é›†è¿˜ä¸ºç©ºæ—¶ï¼Œä¸ç”¨æ‹¼æ¥è€Œæ˜¯å°†å½“å‰æŠ˜èµ‹ç»™è®­ç»ƒæ•°æ®é›†
                X_train, y_train = X_part, y_part
            else:
                X_train = tf.concat([X_train, X_part], 0) # æ‹¼æ¥å½“å‰æŠ˜çš„å¼ é‡åˆ°è®­ç»ƒæ•°æ®é›†ï¼Œåœ¨ç¬¬0ç»´ï¼Œå³æŒ‰è¡Œæ‹¼æ¥
                y_train = tf.concat([y_train, y_part], 0)
        return X_train, y_train, X_valid, y_valid # è¿”å›k-1æŠ˜åˆå¹¶çš„è®­ç»ƒæ•°æ®é›†å’Œå…¶ä¸­ä¸€æŠ˜ç»„æˆçš„éªŒè¯æ•°æ®é›†
```

* å½“æˆ‘ä»¬åœ¨ ğ¾ æŠ˜äº¤å‰éªŒè¯ä¸­è®­ç»ƒ ğ¾ æ¬¡åï¼Œè¿”å›è®­ç»ƒå’ŒéªŒè¯è¯¯å·®çš„å¹³å‡å€¼ã€‚ä¸‹é¢å®šä¹‰è¿ç”¨KæŠ˜äº¤å‰éªŒè¯çš„è®­ç»ƒå‡½æ•°ã€‚
* [å‡½æ•°è°ƒç”¨æ—¶*è¿ç®—ç¬¦çš„ä½œç”¨](https://blog.csdn.net/cadi2011/article/details/84871401)


```python
def k_fold(k, X_train, y_train, num_epochs, learnging_rate, weight_decay, batch_size):
    train_l_sum, valid_l_sum =0, 0
    for i in range(k): # è¿›è¡Œkæ¬¡kæŠ˜è®­ç»ƒï¼Œæ¯ä¸€æ¬¡è®­ç»ƒè¿­ä»£num_epochsæ¬¡ï¼ˆå³æ¯ä¸€ä¸ªtrainå‡½æ•°å†…éƒ¨ä¼šè¿­ä»£num_epochsæ¬¡è¿›è¡Œå‚æ•°çš„ä¼˜åŒ–ï¼‰
        data = get_k_fold_data(k, i, X_train, y_train) # è·å–ç»è¿‡kæŠ˜å¤„ç†çš„æ•°æ®ï¼ˆè®­ç»ƒå’ŒéªŒè¯é›†ï¼‰
        net = get_net() # è·å–å®šä¹‰çš„æ¨¡å‹
        # è®­ç»ƒæ•°æ®å¹¶è·å–è¿”å›çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ç›¸å¯¹è¯¯å·®å€¼æ•°ç»„
        train_ls, valid_ls = train(net, *data, num_epochs, learnging_rate, weight_decay, batch_size) # *dataå°†dataæ•°æ®ä¸€ä¸€è§£å°
        train_l_sum += train_ls[-1]
        valid_l_sum += valid_ls[-1] # å–è®­ç»ƒåç›¸å¯¹è¯¯å·®çš„æœ€åä¸€ä½ï¼Œå³ä¸€æ¬¡è®­ç»ƒç»“æŸåçš„ç›¸å¯¹è¯¯å·®
#         if i == 0: # å°†ç¬¬ä¸€æ¬¡kæŠ˜è®­ç»ƒçš„æƒ…å†µç»˜åˆ¶æˆå›¾åƒå±•ç¤º
#             d2l.plot(list(range(1, num_epochs+1)), [train_ls, valid_ls],
#                     xlabel='epoch', ylabel='rmse', xlim=[1, num_epochs],
#                     legend=['train', 'valid'], yscale='log')
#         print(f'fold {i + 1} train log rmse {float(train_ls[-1]):f},'
#               f'valid log rmse {float(valid_ls[-1]):f}') # è¾“å‡ºæ¯ä¸€æ¬¡kæŠ˜è®­ç»ƒåè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ç›¸å¯¹è¯¯å·®ï¼Œ:fæ˜¯é™åˆ¶æ•°æ®è¾“å‡ºçš„æ ¼å¼ä¿ç•™å¤šå°‘ä½
    return train_l_sum / k, valid_l_sum / k # è¿”å›kæ¬¡kæŠ˜è®­ç»ƒåçš„ç›¸å¯¹è¯¯å·®å¹³å‡å€¼
```

* ä¸€æ¬¡KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒçš„ç¤ºä¾‹ï¼š


```python
# å®šä¹‰è¶…å‚æ•°
k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64
# 5æ¬¡kæŠ˜è®­ç»ƒ
train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr, weight_decay, batch_size)
print(f'{k}-æŠ˜éªŒè¯: å¹³å‡è®­ç»ƒlog rmse: {float(train_l):f}, '
      f'å¹³å‡éªŒè¯log rmse: {float(valid_l):f}')
```

    fold 1 train log rmse 0.170829,valid log rmse 0.157123
    fold 2 train log rmse 0.162197,valid log rmse 0.188655
    fold 3 train log rmse 0.164240,valid log rmse 0.168701
    fold 4 train log rmse 0.168455,valid log rmse 0.154722
    fold 5 train log rmse 0.163307,valid log rmse 0.182933
    5-æŠ˜éªŒè¯: å¹³å‡è®­ç»ƒlog rmse: 0.165806, å¹³å‡éªŒè¯log rmse: 0.170427




![](https://zjpicture.oss-cn-beijing.aliyuncs.com/img/20211127121236.svg)
    


* ä¸‹é¢é€šè¿‡kæŠ˜äº¤å‰éªŒè¯æ¥ä»12åˆ°24é€‰æ‹©ä¸€ä¸ªæœ€ä¼˜çš„å­¦ä¹ ç‡ï¼ˆlrï¼‰è¶…å‚æ•°ï¼š


```python
# å…¶å®ƒè¶…å‚æ•°å›ºå®šä¸å˜
k, num_epochs, weight_decay, batch_size = 5, 100,0, 64
for lr in range(12, 25, 1): # å­¦ä¹ ç‡ä»12åˆ°24å–å€¼
    train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr, weight_decay, batch_size)
    print(f'å­¦ä¹ ç‡ {lr}: å¹³å‡è®­ç»ƒlog rmse: {float(train_l):f}, '
      f'å¹³å‡éªŒè¯log rmse: {float(valid_l):f}')
```

    å­¦ä¹ ç‡ 20: å¹³å‡è®­ç»ƒlog rmse: 0.130166, å¹³å‡éªŒè¯log rmse: 0.148109
    å­¦ä¹ ç‡ 21: å¹³å‡è®­ç»ƒlog rmse: 0.129168, å¹³å‡éªŒè¯log rmse: 0.148078
    å­¦ä¹ ç‡ 22: å¹³å‡è®­ç»ƒlog rmse: 0.128741, å¹³å‡éªŒè¯log rmse: 0.148535
    å­¦ä¹ ç‡ 23: å¹³å‡è®­ç»ƒlog rmse: 0.128011, å¹³å‡éªŒè¯log rmse: 0.147719
    å­¦ä¹ ç‡ 24: å¹³å‡è®­ç»ƒlog rmse: 0.127184, å¹³å‡éªŒè¯log rmse: 0.148102
    å­¦ä¹ ç‡ 25: å¹³å‡è®­ç»ƒlog rmse: 0.127095, å¹³å‡éªŒè¯log rmse: 0.148823
    å­¦ä¹ ç‡ 26: å¹³å‡è®­ç»ƒlog rmse: 0.126952, å¹³å‡éªŒè¯log rmse: 0.149455
    å­¦ä¹ ç‡ 27: å¹³å‡è®­ç»ƒlog rmse: 0.126383, å¹³å‡éªŒè¯log rmse: 0.150066
    å­¦ä¹ ç‡ 28: å¹³å‡è®­ç»ƒlog rmse: 0.125802, å¹³å‡éªŒè¯log rmse: 0.149238
    å­¦ä¹ ç‡ 29: å¹³å‡è®­ç»ƒlog rmse: 0.125103, å¹³å‡éªŒè¯log rmse: 0.148572
    å­¦ä¹ ç‡ 30: å¹³å‡è®­ç»ƒlog rmse: 0.125125, å¹³å‡éªŒè¯log rmse: 0.149642


* ä»ä¸Šé¢å¯ä»¥åˆ†æå‡ºï¼Œåœ¨è¯¥èŒƒå›´å†…éšç€å­¦ä¹ ç‡çš„ä¸Šå‡ï¼Œå¹³å‡è®­ç»ƒè¯¯å·®ä¸€ç›´ä¸‹é™ï¼›
* ä½†æ˜¯æˆ‘ä»¬è¦æ³¨æ„åˆ°ï¼Œåœ¨å­¦ä¹ ç‡è¶…è¿‡23ä¹‹åï¼Œåœ¨å¹³å‡è®­ç»ƒè¯¯å·®ä¸‹é™çš„åŒæ—¶ï¼Œå¹³å‡éªŒè¯è¯¯å·®å´æœ‰ä¸Šå‡ã€‚
    * è¿™è¯´æ˜æˆ‘ä»¬åœ¨å­¦ä¹ ç‡è¶…è¿‡23ä¹‹åï¼Œæˆ‘ä»¬çš„å¹³å‡è®­ç»ƒè¯¯å·®çš„ä¸Šå‡ï¼Œæ˜¯ä»¥è¿‡æ‹Ÿåˆä¸ºä»£ä»·çš„ã€‚
    * æ‰€ä»¥ï¼Œæˆ‘ä»¬åº”è¯¥é€‰æ‹©23ä¸ºå­¦ä¹ ç‡ã€‚

* ç¡®å®šæˆ‘ä»¬è¦é€‰æ‹©çš„æœ€ä¼˜è¶…å‚æ•°ä¹‹åï¼Œæˆ‘ä»¬å†ç”¨æ‰€æœ‰æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚
* ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡è¿™ç§æ–¹å¼è·å¾—çš„æ¨¡å‹å¯ä»¥åº”ç”¨äºæµ‹è¯•é›†ã€‚
* å°†é¢„æµ‹ä¿å­˜åœ¨CSVæ–‡ä»¶ä¸­ï¼Œå°†ç»“æœä¸Šä¼ åˆ°Kaggleã€‚


```python
# å®šä¹‰è®­ç»ƒå’Œé¢„æµ‹å‡½æ•°
def train_and_pred(train_featrues, test_features, train_labels, test_data, 
                  num_epochs, lr, weight_decay, batch_size):
    net = get_net()
    # éªŒè¯é›†ç›¸å¯¹è¯¯å·®æ²¡æœ‰ç”¨ï¼Œç”¨ä¸€ä¸ª_æ¥å—ä¹‹åå°±ä¸ç”¨ç®¡äº†
    train_ls, _ = train(net, train_features, train_labels, None, None,
                        num_epochs, lr, weight_decay, batch_size) # ä¸¤ä¸ªNoneåŸæ¥æ˜¯ä¼ çš„éªŒè¯é›†ï¼Œæ­¤å¤„ä¸éœ€è¦
    # ç»˜åˆ¶è®­ç»ƒé›†ç›¸å¯¹è¯¯å·®éšç€æ¨¡å‹è®­ç»ƒè¿­ä»£æ¬¡æ•°çš„å¢åŠ çš„å˜åŒ–å›¾
    d2l.plot(np.arange(1, num_epochs + 1), [train_ls], xlabel='epoch',
             ylabel='log rmse', xlim=[1, num_epochs], yscale='log')
    # è¾“å‡ºæ¨¡å‹è®­ç»ƒç»“æŸåçš„è®­ç»ƒé›†ç›¸å¯¹è¯¯å·®
    print(f'train log rmse {float(train_ls[-1]):f}')
    # å°†æ¨¡å‹è¿ç”¨äºæµ‹è¯•é›†
    preds = net(test_features).numpy()
    # å°†å…¶é‡æ–°æ ¼å¼åŒ–ä»¥å¯¼å‡ºåˆ°Kaggle
    test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])
    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)
    submission.to_csv('submission.csv', index=False)
```

* ä¸‹é¢ä»£ç åœ¨æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå¹¶é¢„æµ‹æµ‹è¯•é›†ä¸Šçš„æ•°æ®ï¼Œå°†é¢„æµ‹ç»“æœç”Ÿæˆä¸€ä¸ªåä¸ºsubmission.csvçš„æ–‡ä»¶ã€‚


```python
# è®¾ç½®è¶…å‚æ•°
num_epochs, lr, weight_decay, batch_size = 100, 23, 0, 64
train_and_pred(train_features, test_features, train_labels, test_data,
               num_epochs, lr, weight_decay, batch_size)
```

    train log rmse 0.127216




![](https://zjpicture.oss-cn-beijing.aliyuncs.com/img/20211127121255.svg)
    


# æäº¤é¢„æµ‹æ•°æ®

* ç”Ÿæˆçš„é¢„æµ‹æ•°æ®æ ·å¼ï¼š
    ![R2uWkc](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/R2uWkc.png)

* æäº¤åˆ°kaggleç»“æœï¼š
    ![BHSK4g](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/BHSK4g.png)
* å…¶ä¸­çš„scoreå°±æ˜¯ä»£è¡¨é¢„æµ‹å€¼ä¸å®é™…å€¼çš„è¯¯å·®ï¼Œè¶Šå°è¶Šå¥½ã€‚ï¼ˆè¿™é‡Œçš„scoreè®¡ç®—æ–¹æ³•ï¼Œå°±æ˜¯æˆ‘ä»¬è®¡ç®—çš„å¯¹æ•°ç›¸å¯¹è¯¯å·®ï¼‰
* é¡ºä¾¿çœ‹ä¸ªæ’åï¼š
    ![dWvv0J](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/dWvv0J.png)

