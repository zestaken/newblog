---
title: 深度学习六——多层感知机
date: 2021-11-18 15:13:19
tags: [深度学习]
categories: 技术笔记
---
# 多层感知机

* 最简单的深度网络称为多层感知机，它们由多层神经元组成，每一层都与下面一层（从中接收输入）和上面一层（反过来影响当前层的神经元）完全相连。
* 当我们训练大容量模型时，我们面临着过拟合的风险。因此，我们需要将了解包括过拟合、欠拟合和模型选择。为了解决这些问题，我们将使用权重衰减和dropout等正则化技术。

## 隐藏层


### 线性模型的缺陷

* 线性意味着*单调*假设：特征的任何增大都会导致模型输出增大（如果对应的权重为正），或者导致模型输出减少（如果对应的权重为负）。
* 但是这种单调假设不一定始终成立：例如，如何对猫和狗的图像进行分类呢？增加位置(13, 17)处像素的强度是否总是增加(或总是降低)图像描绘狗的可能性？对线性模型的依赖对应于一个隐含的假设，即区分猫和狗的唯一要求是评估单个像素的强度。
    * 这里的线性很荒谬，而且我们难以通过简单的预处理来解决这个问题。这是因为任何像素的重要性都以复杂的方式取决于该像素的上下文（周围像素的值）。我们的数据可能会有一种表示，这种表示会考虑到我们的特征之间的相关交互作用。

### 在网络中加入隐藏层

* 隐藏层与多层感知机：我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。要做到这一点，最简单的方法是将许多全连接层堆叠在一起。每一层都输出到上面的层，直到生成最后的输出。我们可以把前$L-1$层看作表示，把最后一层看作线性预测器。这种架构通常称为*多层感知机*（multilayer perceptron），通常缩写为*MLP*。下面，我们以图的方式描述了多层感知机。

![tvOXSN](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/tvOXSN.png)

* 这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算；因此，这个多层感知机中的层数为2。注意，这两个层都是全连接的。每个输入都会影响隐藏层中的每个神经元，而隐藏层中的每个神经元又会影响输出层中的每个神经元。

### 从线性到非线性

* 我们通过矩阵$\mathbf{X} \in \mathbb{R}^{n \times d}$来表示$n$个样本的小批量，其中每个样本具有$d$个输入(特征)。对于具有$h$个隐藏单元的单隐藏层多层感知机，用$\mathbf{H} \in \mathbb{R}^{n \times h}$表示隐藏层的输出，称为*隐藏表示*（hidden representations）。在数学或代码中，$\mathbf{H}$也被称为*隐藏层变量*（hidden-layer variable）或*隐藏变量*（hidden variable）。
* 因为隐藏层和输出层都是全连接的，所以我们具有隐藏层权重$\mathbf{W}^{(1)} \in \mathbb{R}^{d \times h}$和隐藏层偏置$\mathbf{b}^{(1)} \in \mathbb{R}^{1 \times h}$以及输出层权重$\mathbf{W}^{(2)} \in \mathbb{R}^{h \times q}$和输出层偏置$\mathbf{b}^{(2)} \in \mathbb{R}^{1 \times q}$。形式上，我们按如下方式计算单隐藏层多层感知机的输出$\mathbf{O} \in \mathbb{R}^{n \times q}$：

    $$
    \begin{aligned}
        \mathbf{H} & = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \\
        \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.
    \end{aligned}
    $$

* 为了发挥多层结构的潜力，我们还需要一个额外的关键要素：在仿射变换之后对每个隐藏单元应用非线性的*激活函数*（activation function）$\sigma$。激活函数的输出（例如，$\sigma(\cdot)$）被称为*激活值*（activations）。一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型：

    $$
    \begin{aligned}
        \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\
        \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\
    \end{aligned}
    $$
* 我们应用于隐藏层的激活函数通常不仅仅是按行的，而且也是按元素。这意味着在计算每一层的线性部分之后，我们可以计算每个激活值，而不需要查看其他隐藏单元所取的值。对于大多数激活函数都是这样。为了构建更通用的多层感知机，我们可以继续堆叠这样的隐藏层，例如，$\mathbf{H}^{(1)} = \sigma_1(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})$和$\mathbf{H}^{(2)} = \sigma_2(\mathbf{H}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)})$，一层叠一层，从而产生更有表达能力的模型。

### 通用近似定理

* 多层感知机可以通过隐藏神经元捕捉到我们输入之间复杂的相互作用，这些神经元依赖于每个输入的值。我们可以很容易地设计隐藏节点来执行任意计算。例如，在一对输入上进行基本逻辑操作。多层感知机是通用近似器。即使是网络只有一个隐藏层，给定足够的神经元（可能非常多）和正确的权重，我们可以对任意函数建模，尽管实际中学习该函数是很困难的。你可能认为神经网络有点像C语言。C语言和任何其他现代编程语言一样，能够表达任何可计算的程序。但实际上，想出一个符合规范的程序才是最困难的部分。

## 激活函数

* 激活函数通过计算加权和并加上偏置来确定神经元是否应该被激活。它们是将输入信号转换为输出的可微运算。大多数激活函数都是非线性的。由于激活函数是深度学习的基础，下面(**简要介绍一些常见的激活函数**)。


```python
import matplotlib.pyplot as plt
import tensorflow as tf
```

### ReLU函数

* 最受欢迎的选择是*线性整流单元*（Rectified linear unit，*ReLU*），因为它实现简单，同时在各种预测任务中表现良好。[**ReLU提供了一种非常简单的非线性变换**]。给定元素$x$，ReLU函数被定义为该元素与$0$的最大值：

    $$\operatorname{ReLU}(x) = \max(x, 0).$$

* 通俗地说，ReLU函数通过将相应的激活值设为0来仅保留正元素并丢弃所有负元素。为了直观感受一下，我们可以画出函数的曲线图。正如从图中所看到，激活函数是分段线性的。


```python
x = tf.Variable(tf.range(-8.0, 8.0, 0.1), dtype=tf.float32)
y = tf.nn.relu(x)
plt.plot(x.numpy(), y.numpy())
```




    [<matplotlib.lines.Line2D at 0x7fcada8f47c0>]



<img src="https://gitee.com/zhangjie0524/picgo/raw/master/uPic/1DBtwy.png" alt="1DBtwy" style="zoom:50%;" />    

​    


* 当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1。注意，当输入值精确等于0时，ReLU函数不可导。在此时，我们默认使用左侧的导数，即当输入为0时导数为0。我们可以忽略这种情况，因为输入可能永远都不会是0。下面我们绘制ReLU函数的导数。


```python
with tf.GradientTape() as t:
    y = tf.nn.relu(x)
plt.plot(x.numpy(), t.gradient(y, x).numpy())
```




    [<matplotlib.lines.Line2D at 0x7fcadaa52310>]




![output_10_1](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/output_10_1.png)
    


* 使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题。

* 注意，ReLU函数有许多变体，包括*参数化ReLU*（Parameterized ReLU，*pReLU*）函数。该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：

    $$\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x).$$

### sigmoid函数
* [**对于一个定义域在$\mathbb{R}$中的输入，*sigmoid函数*将输入变换为区间(0, 1)上的输出**]。因此，sigmoid通常称为*挤压函数*（squashing function）：它将范围(-inf, inf)中的任意输入压缩到区间(0, 1)中的某个值：

    $$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.$$
* 当人们的注意力逐渐转移到基于梯度的学习时，sigmoid函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。当我们想要将输出视作二分类问题的概率时，sigmoid仍然被广泛用作输出单元上的激活函数（你可以将sigmoid视为softmax的特例）。然而，sigmoid在隐藏层中已经较少使用，它在大部分时候已经被更简单、更容易训练的ReLU所取代。
* 我们绘制sigmoid函数。注意，当输入接近0时，sigmoid函数接近线性变换。


```python
x = tf.Variable(tf.range(-8.0, 8.0, 0.1), dtype=tf.float32)
y = tf.nn.sigmoid(x)
plt.plot(x.numpy(), y.numpy())
```




    [<matplotlib.lines.Line2D at 0x7fcadab24280>]




![output_13_1](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/output_13_1.png)
    


* sigmoid函数的导数为下面的公式：

    $$\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right).$$
* sigmoid函数的导数图像如下所示。注意，当输入为0时，sigmoid函数的导数达到最大值0.25。而输入在任一方向上越远离0点，导数越接近0。


```python
with tf.GradientTape() as t:
    y = tf.nn.sigmoid(x)
plt.plot(x.numpy(), t.gradient(y, x).numpy())
```




    [<matplotlib.lines.Line2D at 0x7fcadac1df10>]




![output_15_1](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/output_15_1.png)
    


### tanh函数

* 与sigmoid函数类似，[**tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上**]。tanh函数的公式如下：

    $$\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.$$

* 下面我们绘制tanh函数。注意，当输入在0附近时，tanh函数接近线性变换。函数的形状类似于sigmoid函数，不同的是**tanh函数关于坐标系原点中心对称**。


```python
x = tf.Variable(tf.range(-8.0, 8.0, 0.1), dtype=tf.float32)
y = tf.nn.tanh(x)
plt.plot(x.numpy(), y.numpy())
```




    [<matplotlib.lines.Line2D at 0x7fcadad0f040>]


![output_17_1](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/output_17_1.png)
    


* tanh函数的导数是：

    $$\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x).$$

* tanh函数的导数图像如下所示。当输入接近0时，tanh函数的导数接近最大值1。与我们在sigmoid函数图像中看到的类似，输入在任一方向上越远离0点，导数越接近0。


```python
with tf.GradientTape() as t:
    y = tf.nn.tanh(x)
plt.plot(x.numpy(), t.gradient(y, x).numpy())
```




    [<matplotlib.lines.Line2D at 0x7fcadadf2c10>]


![output_19_1](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/output_19_1.png)
    

# 多层感知机的从零开始实现

* 我们已经在数学上描述了多层感知机（MLP），现在让我们尝试自己实现一个多层感知机。为了与我们之前使用softmax回归（ 获得的结果进行比较，我们将继续使用Fashion-MNIST图像分类数据集（ :numref:`sec_fashion_mnist`）。


```python
import tensorflow as tf
import matplotlib.pyplot as plt
```


```python
batch_size = 256
# 加载数据
def load_data_fashion_mnist(batch_size, resize=None):   
    """下载Fashion-MNIST数据集，然后将其加载到内存中。"""
    mnist_train, mnist_test = tf.keras.datasets.fashion_mnist.load_data()
    # 将所有数字除以255，使所有像素值介于0和1之间，在最后添加一个批处理维度，
    # 并将标签转换为int32。
    process = lambda X, y: (tf.expand_dims(X, axis=3) / 255,
                            tf.cast(y, dtype='int32'))
    resize_fn = lambda X, y: (
        tf.image.resize_with_pad(X, resize, resize) if resize else X, y)
    return (
        tf.data.Dataset.from_tensor_slices(process(*mnist_train)).batch(
            batch_size).shuffle(len(mnist_train[0])).map(resize_fn),
        tf.data.Dataset.from_tensor_slices(process(*mnist_test)).batch(
            batch_size).map(resize_fn))
# 加载训练集和测试集
train_iter, test_iter = load_data_fashion_mnist(batch_size)
```

## 初始化模型参数

* Fashion-MNIST中的每个图像由$28 \times 28 = 784$个灰度像素值组成。所有图像共分为10个类别。忽略像素之间的空间结构，我们可以将每个图像视为具有784个输入特征和10个类的简单分类数据集。
* 首先，我们将[**实现一个具有单隐藏层的多层感知机，它包含256个隐藏单元**]。注意，我们可以将这两个量都视为超参数。通常，我们选择2的若干次幂作为层的宽度(即有两层)。因为内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。
* 我们用几个张量来表示我们的参数。注意，对于每一层我们都要记录一个权重矩阵和一个偏置向量。跟以前一样，我们要为这些参数的损失的梯度分配内存。


```python
num_inputs, num_outputs, num_hiddens = 784, 10, 256

# 因为有两层隐藏层，所以有两组参数
W1 = tf.Variable(tf.random.normal(
    shape=(num_inputs, num_hiddens), mean=0, stddev=0.01))
b1 = tf.Variable(tf.zeros(num_hiddens)) # 一个隐藏单元对应一个转换函数，对应一个参数
W2 = tf.Variable(tf.random.normal(
    shape=(num_hiddens, num_outputs), mean=0, stddev=0.01))
b2 = tf.Variable(tf.random.normal([num_outputs], stddev=.01))

params = [W1, b1, W2, b2]
```

## 激活函数

* 为了确保我们知道一切是如何工作的，我们将使用最大值函数自己[**实现ReLU激活函数**]，而不是直接调用内置的`relu`函数。


```python
def relu(X):
    return tf.math.maximum(X, 0)
```

## 模型

* 因为我们忽略了空间结构，所以我们使用`reshape`将每个二维图像转换为一个长度为`num_inputs`的向量。我们只需几行代码就可以(**实现我们的模型**)。


```python
def net(X):
    X = tf.reshape(X, (-1, num_inputs)) # 输入变量，特征值
    H = relu(tf.matmul(X, W1) + b1) # relu隐藏层
    return tf.matmul(H, W2) + b2 # 输出函数
```

## 损失函数

* 为了确保数值稳定性，同时由于我们已经从零实现过softmax函数，因此在这里我们直接使用高级API中的内置函数来计算softmax和交叉熵损失。


```python
def loss(y_hat, y):
    return tf.losses.sparse_categorical_crossentropy(
        y, y_hat, from_logits=True)
```

## 训练

* [**多层感知机的训练过程与softmax回归的训练过程完全相同**]。可以使用之前定义的`train_ch3`函数将迭代周期数设置为10，并将学习率设置为0.1.


```python
class Accumulator:  
    """在`n`个变量上累加。"""
    def __init__(self, n):
        self.data = [0.0] * n # 乘法可以将向量延展n倍，如n=2则变为[0.0, 0.0]

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)] # 合并成一个新元组如[[0,a],[1, b]]再遍历

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
    
def accuracy(y_hat, y):  
    """计算预测正确的数量。"""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = tf.argmax(y_hat, axis=1) # 获取矩阵中每一行的最大元素的索引值
    cmp = tf.cast(y_hat, y.dtype) == y # 类型转换后再比较
    # 比较后矩阵中再求和，一位代表一行，如果该行匹配，则值为1
    return float(tf.reduce_sum(tf.cast(cmp, y.dtype))) 

def evaluate_accuracy(net, data_iter):  
    """计算在指定数据集上模型的精度。"""
    # 参数2是指，一个有两个元素的向量，即：第0位为模型预测数，第1位为预测的数据总数
    metric = Accumulator(2)  
    for X, y in data_iter:
        metric.add(accuracy(net(X), y), len(y))
    return metric[0] / metric[1]    # 第一个是准确数，第二个是预测总数 

def train_epoch_ch3(net, train_iter, loss, updater):  
    """训练模型一个迭代周期"""
    # 训练损失总和、训练准确度总和、样本数
    metric = Accumulator(3)
    for X, y in train_iter:
        # 计算梯度并更新参数
        with tf.GradientTape() as tape:
            y_hat = net(X)
            # Keras内置的损失接受的是（标签，预测），这不同于用户在本书中的实现。
            # 本书的实现接受（预测，标签），例如我们上面实现的“交叉熵”
            if isinstance(loss, tf.keras.losses.Loss): # 检测是否是指定的函数类型
                l = loss(y, y_hat)
            else:
                l = loss(y_hat, y)
        if isinstance(updater, tf.keras.optimizers.Optimizer):
            params = net.trainable_variables
            grads = tape.gradient(l, params)
            updater.apply_gradients(zip(grads, params))
        else:
            updater(X.shape[0], tape.gradient(l, updater.params))
        # Keras的`loss`默认返回一个批量的平均损失
        l_sum = l * float(tf.size(y)) if isinstance(
            loss, tf.keras.losses.Loss) else tf.reduce_sum(l)
        metric.add(l_sum, accuracy(y_hat, y), tf.size(y))
    # 返回训练损失和训练准确率
    return metric[0] / metric[2], metric[1] / metric[2]

 # params是元素类型为tf.Varible的列表，grads使参数对应的损失函数导数，lr是学习率
def sgd(params, grads, lr, batch_size): 
    """小批量随机梯度下降。"""
    for param, grad in zip(params, grads): # zip函数将可迭代对象打包成一个个元组用于遍历
        #tensorflow的assign_sub函数能够将param减去括号中参数的值再赋给param，实现更新参数
        param.assign_sub(lr*grad/batch_size) 
        
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  
    """训练模型"""
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc
class Updater():  #@save
    """用小批量随机梯度下降法更新参数。"""
    def __init__(self, params, lr):
        self.params = params
        self.lr = lr

    def __call__(self, batch_size, grads):
        sgd(self.params, grads, self.lr, batch_size)

num_epochs, lr = 10, 0.1
updater = Updater([W1, W2, b1, b2], lr)
train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)
```

* 为了对学习到的模型进行评估，我们将[**在一些测试数据上应用这个模型**]。


```python
def get_fashion_mnist_labels(labels):  # 根据数字代码获取对应的名字
    """返回Fashion-MNIST数据集的文本标签。"""
    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]

def show_images(imgs, num_rows, num_cols, titles=None, scale=3):  # scale设置图像显示的大小 
    """绘制图像列表。"""
    figsize = (num_cols * scale, num_rows * scale)
    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)
    axes = axes.flatten()
    for i, (ax, img) in enumerate(zip(axes, imgs)):
        ax.imshow(img.numpy())
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        if titles:
            ax.set_title(titles[i]) # 从title参数数组中取出对应图像的名称
    return axes

def predict_ch3(net, test_iter, n=6):  
    """预测标签"""
    for X, y in test_iter:
        break
    trues = get_fashion_mnist_labels(y)
    preds = get_fashion_mnist_labels(tf.argmax(net(X), axis=1))
    titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
    show_images(
        tf.reshape(X[0:n], (n, 28, 28)), 1, n, titles=titles[0:n])

predict_ch3(net, test_iter)
```
![output_8_0](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/output_8_0.png)
​    

# 多层感知机的简洁实现

* 我们可以(**通过高级API更简洁地实现多层感知机**)。


```python
import tensorflow as tf
import matplotlib.pyplot as plt
```

## 模型

* 与softmax回归的简洁实现相比，唯一的区别是我们添加了2个全连接层（之前我们只添加了1个全连接层）。第一层是[**隐藏层**]，它(**包含256个隐藏单元，并使用了ReLU激活函数**)。第二层是输出层。


```python
net = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256, activation='relu'), # 隐藏层是由256个单元的，使用relu激活函数的全连接层
    tf.keras.layers.Dense(10)]) # 输出层是10个单元（因为有10个类别）的全连接层
```

* [**训练过程**]的实现与我们实现softmax回归时完全相同，这种模块化设计使我们能够将与和模型架构有关的内容独立出来。


```python
batch_size, lr, num_epochs = 256, 0.1, 10
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # 交叉熵损失函数
trainer = tf.keras.optimizers.SGD(learning_rate=lr) # sgd小批量梯度下降更新函数
```


```python
# 加载数据
def load_data_fashion_mnist(batch_size, resize=None):   
    """下载Fashion-MNIST数据集，然后将其加载到内存中。"""
    mnist_train, mnist_test = tf.keras.datasets.fashion_mnist.load_data()
    # 将所有数字除以255，使所有像素值介于0和1之间，在最后添加一个批处理维度，
    # 并将标签转换为int32。
    process = lambda X, y: (tf.expand_dims(X, axis=3) / 255,
                            tf.cast(y, dtype='int32'))
    resize_fn = lambda X, y: (
        tf.image.resize_with_pad(X, resize, resize) if resize else X, y)
    return (
        tf.data.Dataset.from_tensor_slices(process(*mnist_train)).batch(
            batch_size).shuffle(len(mnist_train[0])).map(resize_fn),
        tf.data.Dataset.from_tensor_slices(process(*mnist_test)).batch(
            batch_size).map(resize_fn))
# 加载训练集和测试集
train_iter, test_iter = load_data_fashion_mnist(batch_size)
class Accumulator:  
    """在`n`个变量上累加。"""
    def __init__(self, n):
        self.data = [0.0] * n # 乘法可以将向量延展n倍，如n=2则变为[0.0, 0.0]

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)] # 合并成一个新元组如[[0,a],[1, b]]再遍历

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
    
def accuracy(y_hat, y):  
    """计算预测正确的数量。"""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = tf.argmax(y_hat, axis=1) # 获取矩阵中每一行的最大元素的索引值
    cmp = tf.cast(y_hat, y.dtype) == y # 类型转换后再比较
    # 比较后矩阵中再求和，一位代表一行，如果该行匹配，则值为1
    return float(tf.reduce_sum(tf.cast(cmp, y.dtype))) 

def evaluate_accuracy(net, data_iter):  
    """计算在指定数据集上模型的精度。"""
    # 参数2是指，一个有两个元素的向量，即：第0位为模型预测数，第1位为预测的数据总数
    metric = Accumulator(2)  
    for X, y in data_iter:
        metric.add(accuracy(net(X), y), len(y))
    return metric[0] / metric[1]    # 第一个是准确数，第二个是预测总数 

def train_epoch_ch3(net, train_iter, loss, updater):  
    """训练模型一个迭代周期"""
    # 训练损失总和、训练准确度总和、样本数
    metric = Accumulator(3)
    for X, y in train_iter:
        # 计算梯度并更新参数
        with tf.GradientTape() as tape:
            y_hat = net(X)
            # Keras内置的损失接受的是（标签，预测），这不同于用户在本书中的实现。
            # 本书的实现接受（预测，标签），例如我们上面实现的“交叉熵”
            if isinstance(loss, tf.keras.losses.Loss): # 检测是否是指定的函数类型
                l = loss(y, y_hat)
            else:
                l = loss(y_hat, y)
        if isinstance(updater, tf.keras.optimizers.Optimizer):
            params = net.trainable_variables
            grads = tape.gradient(l, params)
            updater.apply_gradients(zip(grads, params))
        else:
            updater(X.shape[0], tape.gradient(l, updater.params))
        # Keras的`loss`默认返回一个批量的平均损失
        l_sum = l * float(tf.size(y)) if isinstance(
            loss, tf.keras.losses.Loss) else tf.reduce_sum(l)
        metric.add(l_sum, accuracy(y_hat, y), tf.size(y))
    # 返回训练损失和训练准确率
    return metric[0] / metric[2], metric[1] / metric[2]

 # params是元素类型为tf.Varible的列表，grads使参数对应的损失函数导数，lr是学习率
def sgd(params, grads, lr, batch_size): 
    """小批量随机梯度下降。"""
    for param, grad in zip(params, grads): # zip函数将可迭代对象打包成一个个元组用于遍历
        #tensorflow的assign_sub函数能够将param减去括号中参数的值再赋给param，实现更新参数
        param.assign_sub(lr*grad/batch_size) 
        
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  
    """训练模型"""
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc
class Updater():  #@save
    """用小批量随机梯度下降法更新参数。"""
    def __init__(self, params, lr):
        self.params = params
        self.lr = lr

    def __call__(self, batch_size, grads):
        sgd(self.params, grads, self.lr, batch_size)

train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

* 为了对学习到的模型进行评估，我们将[**在一些测试数据上应用这个模型**]。


```python
def get_fashion_mnist_labels(labels):  # 根据数字代码获取对应的名字
    """返回Fashion-MNIST数据集的文本标签。"""
    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]

def show_images(imgs, num_rows, num_cols, titles=None, scale=3):  # scale设置图像显示的大小 
    """绘制图像列表。"""
    figsize = (num_cols * scale, num_rows * scale)
    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)
    axes = axes.flatten()
    for i, (ax, img) in enumerate(zip(axes, imgs)):
        ax.imshow(img.numpy())
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        if titles:
            ax.set_title(titles[i]) # 从title参数数组中取出对应图像的名称
    return axes

def predict_ch3(net, test_iter, n=6):  
    """预测标签"""
    for X, y in test_iter:
        break
    trues = get_fashion_mnist_labels(y)
    preds = get_fashion_mnist_labels(tf.argmax(net(X), axis=1))
    titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
    show_images(
        tf.reshape(X[0:n], (n, 28, 28)), 1, n, titles=titles[0:n])

predict_ch3(net, test_iter)
```



![output_8_0](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/output_8_0.png)
    


