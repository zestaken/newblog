---
title: 深度学习七——模型选择、欠拟合和过拟合
date: 2021-11-18 16:03:19
tags: [深度学习]
categories: 技术笔记
---
# 模型选择、欠拟合和过拟合

* **泛化模式**：作为机器学习科学家，我们的目标是发现*模式*（pattern）。但是，我们如何才能确定模型是真正发现了一种泛化的模式，而不是简单地记住了数据呢？例如，我们想要在患者的基因数据与痴呆状态之间寻找模式，其中标签是从集合$\{\text{痴呆}, \text{轻度认知障碍}, \text{健康}\}$中提取的。因为基因可以唯一确定每个个体（不考虑双胞胎），所以在这个任务中是有可能记住整个数据集的。当我们部署模型时，模型会遇到从未见过的数据。只有当我们的模型真正发现了一种泛化模式时，才会作出有效的预测。
* **过拟合与正则化**：将模型在训练数据上拟合得比在潜在分布中更接近的现象称为*过拟合*（overfitting），用于对抗过拟合的技术称为*正则化*（regularization）。在实验中调整模型结构或超参数时，如果有足够多的神经元、层数和训练迭代周期，模型最终可以在训练集上达到完美的精度，此时测试集的准确性却下降了。

## 训练误差和泛化误差

* **训练误差（training error）**：我们的模型在训练数据集上计算得到的误差。
* **泛化误差（generalization error）**：当我们将模型应用在同样从原始样本的分布中抽取的无限多的数据样本时，我们模型误差的期望。
    * 我们永远不能准确地计算出泛化误差。这是因为无限多的数据样本是一个虚构的对象。在实际中，我们只能通过将模型应用于一个独立的测试集来*估计*泛化误差，该测试集由随机选取的、未曾在训练集中出现的数据样本构成。
    * 一个类别示例：假设一个大学生正在努力准备期末考试。一个勤奋的学生会努力做好练习，并利用往年的考试题目来测试自己的能力。尽管如此，在过去的考试题目上取得好成绩并不能保证他会在真正考试时发挥出色。例如，学生可能试图通过死记硬背考题的答案来做准备。他甚至可以完全记住过去考试的答案。另一名学生可能会通过试图理解给出某些答案的原因来做准备。在大多数情况下，后者会考得更好。

### 独立同分布假设

* **独立同分布假设**：在一般标准的监督学习中，我们假设训练数据和测试数据都是从*相同的*分布中*独立*提取的。这通常被称为*独立同分布假设*，这意味着对数据进行采样的过程没有进行“记忆”。换句话说，抽取的第2个样本和第3个样本的相关性并不比抽取的第2个样本和第200万个样本的相关性更强。
* 独立同分布假设经常出现失效的情况，有些违背独立同分布假设的行为肯定会带来麻烦。比如，我们试图只用来自大学生的人脸数据来训练一个人脸识别系统，然后想要用它来监测疗养院中的老人。这不太可能有效，因为大学生看起来往往与老年人有很大的不同。

### 模型复杂性

* 当我们有简单的模型和大量的数据时，我们期望泛化误差与训练误差相近。
* 当我们有更复杂的模型和更少的样本时，我们预计训练误差会下降，但泛化误差会增大。
* 模型复杂性由什么构成是一个复杂的问题。一个模型是否能很好地泛化取决于很多因素。例如，具有更多参数的模型可能被认为更复杂。参数有更大取值范围的模型可能更为复杂。通常，对于神经网络，我们认为需要更多训练迭代的模型比较复杂，而需要“提前停止”（early stopping）的模型（意味着具有较少训练迭代周期）就不那么复杂。
* 几个影响模型泛化的因素：
    * 可调整参数的数量。当可调整参数的数量（有时称为*自由度*）很大时，模型往往更容易过拟合。
    * 参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。
    * 训练样本的数量。即使你的模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。

## 模型选择

* 在机器学习中，我们通常在评估几个候选模型后选择最终的模型。这个过程叫做*模型选择*。有时，需要进行比较的模型在本质上是完全不同的（比如，决策树与线性模型）。又有时，我们需要比较不同的超参数设置下的同一类模型。
* 例如，训练多层感知机模型时，我们可能希望比较具有不同数量的隐藏层、不同数量的隐藏单元以及不同的的激活函数组合的模型。为了确定候选模型中的最佳模型，我们通常会使用验证集。

### 验证集

* **测试集的使用场景**：原则上，在我们确定所有的超参数之前，我们不应该用到测试集。如果我们在模型选择过程中使用测试数据，可能会有过拟合测试数据的风险。那我们就麻烦大了。如果我们过拟合了训练数据，还有在测试数据上的评估来判断过拟合。但是如果我们过拟合了测试数据，我们又该怎么知道呢？
    * 我们决不能依靠测试数据进行模型选择。然而，我们也不能仅仅依靠训练数据来选择模型，因为我们无法估计训练数据的泛化误差。
* **验证集**：在实际应用中，情况变得更加复杂。虽然理想情况下我们只会使用测试数据一次，以评估最好的模型或比较一些模型效果，但现实是，测试数据很少在使用一次后被丢弃。我们很少能有充足的数据来对每一轮实验采用全新测试集。解决此问题的常见做法是将我们的数据分成三份，除了训练和测试数据集之外，还增加一个*验证数据集*（validation dataset），也叫*验证集*（validation set）。

### $K$折交叉验证

* 当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集。这个问题的一个流行的解决方案是采用$K$折交叉验证。
* **K折交叉验证**：原始训练数据被分成$K$个不重叠的子集。然后执行$K$次模型训练和验证，每次在$K-1$个子集上进行训练，并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。最后，通过对$K$次实验的结果取平均来估计训练和验证误差。

## 欠拟合还是过拟合？

* **欠拟合**：训练误差和验证误差都很严重，但它们之间仅有一点差距。如果模型不能降低训练误差，这可能意味着我们的模型过于简单（即表达能力不足），无法捕获我们试图学习的模式。此外，由于我们的训练和验证误差之间的*泛化误差*很小，我们有理由相信可以用一个更复杂的模型降低训练误差。这种现象被称为*欠拟合*（underfitting）。
* **过拟合**：当我们的训练误差明显低于验证误差时，这表明严重的*过拟合*（overfitting）。注意，*过拟合*并不总是一件坏事。特别是在深度学习领域，众所周知，最好的预测模型在训练数据上的表现往往比在保留数据上好得多。最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。
* 我们是否过拟合或欠拟合可能取决于*模型复杂性*和*可用训练数据集*的大小。

### 模型复杂性

* 为了说明一些关于过拟合和模型复杂性的经典的直觉，我们给出一个多项式的例子。给定由单个特征$x$和对应实数标签$y$组成的训练数据，我们试图找到下面的$d$阶多项式来估计标签$y$。

    $$\hat{y}= \sum_{i=0}^d x^i w_i$$

    * 这只是一个线性回归问题，我们的特征是$x$的幂给出的，模型的权重是$w_i$给出的，偏置是$w_0$给出的（因为对于所有的$x$都有$x^0 = 1$）。由于这只是一个线性回归问题，我们可以使用平方误差作为我们的损失函数。
* 高阶多项式函数比低阶多项式函数复杂得多。高阶多项式的参数较多，模型函数的选择范围较广。因此在固定训练数据集的情况下，高阶多项式函数相对于低阶多项式的训练误差应该始终更低（最坏也是相等）。事实上，当数据样本包含了$x$的不同值时，函数阶数等于数据样本数量的多项式函数可以完美拟合训练集。在下图中，我们直观地描述了多项式的阶数和欠拟合与过拟合之间的关系。

![kEwY89](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/kEwY89.png)

### 数据集大小

* 另一个需要牢记的重要因素是数据集的大小。
* 训练数据集中的样本越少，我们就越有可能（且更严重地）遇到过拟合。
* 随着训练数据量的增加，泛化误差通常会减小。此外，一般来说，更多的数据不会有什么坏处。对于固定的任务和数据分布，模型复杂性和数据集大小之间通常存在关系。给出更多的数据，我们可能会尝试拟合一个更复杂的模型。能够拟合更复杂的模型可能是有益的。如果没有足够的数据，简单的模型可能更有用。对于许多任务，深度学习只有在有数千个训练样本时才优于线性模型。


## 多项式回归

* 通过多项式拟合来交互地探索这些概念。


```python
import math
import numpy as np
import tensorflow as tf
from d2l import tensorflow as d2l
```

### 生成数据集

* 首先，我们需要数据。给定$x$，我们将[**使用以下三阶多项式来生成训练和测试数据的标签：**]

    $$y = 5 + 1.2x - 3.4\frac{x^2}{2!} + 5.6 \frac{x^3}{3!} + \epsilon \text{ where }
    \epsilon \sim \mathcal{N}(0, 0.1^2).$$
    * 噪声项$\epsilon$服从均值为0且标准差为0.1的正态分布。
* 在优化的过程中，我们通常希望避免非常大的梯度值或损失值。这就是我们将*特征*从$x^i$调整为$\frac{x^i}{i!}$的原因，这样可以避免很大的$i$带来的特别大的指数值。我们将为训练集和测试集各生成100个样本。存储在`poly_features`中的单项式由gamma函数重新缩放，其中$\Gamma(n)=(n-1)!$（注:[numpy的power函数求幂](https://blog.csdn.net/qq_36512295/article/details/98472358))


```python
max_degree = 20  # 多项式的最大阶数
n_train, n_test = 100, 100  # 训练和测试数据集大小
true_w = np.zeros(max_degree)  # 分配参数的空间
true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])

features = np.random.normal(size=(n_train + n_test, 1)) # 按正态分布生成特征值
np.random.shuffle(features) # 打乱特征值顺序
poly_features = np.power(features, np.arange(max_degree).reshape(1, -1)) # 计算特征值依次的幂,成为一个200x20的矩阵
for i in range(max_degree):
    poly_features[:, i] /= math.gamma(i + 1)  # `ga‘mma(n)` = (n-1)! 给每一个特征值除以对应幂的阶乘
# `labels`的维度: (`n_train` + `n_test`,)
labels = np.dot(poly_features, true_w) # 将特征值和参数相乘起来得到多项式 200x20 20x1 = 200x1的矩阵，每一个元素都是一个多项式之和
labels += np.random.normal(scale=0.1, size=labels.shape) # 加上噪声项
```

* 特征值求幂过程的演示


```python
t1 = np.arange(20).reshape(1, -1)
t2 = np.arange(200).reshape(200, -1)
t3 = np.power(t2, t1)
t3.shape,t3[:,2]  # 每一个特征值的0道20的幂都计算出来，形成一个200x20的矩阵,t3[:,2]是取第二列的所有元素
```




    ((200, 20),
     array([    0,     1,     4,     9,    16,    25,    36,    49,    64,
               81,   100,   121,   144,   169,   196,   225,   256,   289,
              324,   361,   400,   441,   484,   529,   576,   625,   676,
              729,   784,   841,   900,   961,  1024,  1089,  1156,  1225,
             1296,  1369,  1444,  1521,  1600,  1681,  1764,  1849,  1936,
             2025,  2116,  2209,  2304,  2401,  2500,  2601,  2704,  2809,
             2916,  3025,  3136,  3249,  3364,  3481,  3600,  3721,  3844,
             3969,  4096,  4225,  4356,  4489,  4624,  4761,  4900,  5041,
             5184,  5329,  5476,  5625,  5776,  5929,  6084,  6241,  6400,
             6561,  6724,  6889,  7056,  7225,  7396,  7569,  7744,  7921,
             8100,  8281,  8464,  8649,  8836,  9025,  9216,  9409,  9604,
             9801, 10000, 10201, 10404, 10609, 10816, 11025, 11236, 11449,
            11664, 11881, 12100, 12321, 12544, 12769, 12996, 13225, 13456,
            13689, 13924, 14161, 14400, 14641, 14884, 15129, 15376, 15625,
            15876, 16129, 16384, 16641, 16900, 17161, 17424, 17689, 17956,
            18225, 18496, 18769, 19044, 19321, 19600, 19881, 20164, 20449,
            20736, 21025, 21316, 21609, 21904, 22201, 22500, 22801, 23104,
            23409, 23716, 24025, 24336, 24649, 24964, 25281, 25600, 25921,
            26244, 26569, 26896, 27225, 27556, 27889, 28224, 28561, 28900,
            29241, 29584, 29929, 30276, 30625, 30976, 31329, 31684, 32041,
            32400, 32761, 33124, 33489, 33856, 34225, 34596, 34969, 35344,
            35721, 36100, 36481, 36864, 37249, 37636, 38025, 38416, 38809,
            39204, 39601], dtype=int32))




```python
# NumPy ndarray转换为tensor
true_w, features, poly_features, labels = [tf.constant(x, dtype=
    tf.float32) for x in [true_w, features, poly_features, labels]]
```

### 对模型进行训练和测试

* 首先让我们[**实现一个函数来评估模型在给定数据集上的损失**]。其中`Accumulator`类见[自定义函数之模型、损失函数和优化算法中函数](https://www.zestaken.top/post/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0)


```python
def evaluate_loss(net, data_iter, loss):  
    """评估给定数据集上模型的损失。"""
    metric = d2l.Accumulator(2)  # 损失的总和, 样本数量
    for X, y in data_iter:
        l = loss(net(X), y)
        metric.add(tf.reduce_sum(l), d2l.size(l)) # reduce_sum将矩阵的全部元素求和返回一个数值
    return metric[0] / metric[1] # 损失总和除以训练样本数
```

* 现在[**定义训练函数**]。其中`load_array`，`Animator`,`train_epoch_ch3`函数见[自定义函数之生成数据集函数，绘图函数，训练函数](https://www.zestaken.top/post/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0)


```python
def train(train_features, test_features, train_labels, test_labels,
          num_epochs=400):
    loss = tf.losses.MeanSquaredError() # 获取tf自带的均方损失函数
    input_shape = train_features.shape[-1] # 输入形状为特征值矩阵的列数
    # 不设置偏置，因为我们已经在多项式特征中实现了它
    net = tf.keras.Sequential()
    net.add(tf.keras.layers.Dense(1, use_bias=False))
    batch_size = min(10, train_labels.shape[0]) # 小批量样本数
    train_iter = d2l.load_array((train_features, train_labels), batch_size)
    test_iter = d2l.load_array((test_features, test_labels), batch_size,
                               is_train=False)
    trainer = tf.keras.optimizers.SGD(learning_rate=.01)# 小批量sgd梯度损失优化函数
    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log',
                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],
                            legend=['train', 'test'])
    for epoch in range(num_epochs):
        d2l.train_epoch_ch3(net, train_iter, loss, trainer)
        if epoch == 0 or (epoch + 1) % 20 == 0:
            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),
                                     evaluate_loss(net, test_iter, loss)))
    print('weight:', net.get_weights()[0].T)
```

### 三阶多项式函数拟合(正态)

* 通过特征值的维度来控制
* 我们将首先使用三阶多项式函数，它与数据生成函数的*阶数相同*。
* 结果表明，该模型能有效*降低训练损失和测试损失*。学习到的模型参数也接近真实值$w = [5, 1.2, -3.4, 5.6]$。


```python
# 从多项式特征中选择前4个维度，即 1, x, x^2/2!, x^3/3! 即3阶多项式
train(poly_features[:n_train, :4], poly_features[n_train:, :4],
      labels[:n_train], labels[n_train:])
```

    weight: [[ 5.0042057  1.2240036 -3.4093304  5.568672 ]]




![output_22_1](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/output_22_1.svg)
    


###  线性函数拟合(欠拟合)

* 让我们再看看线性函数拟合：线性函数拟合即阶数少于多项式实际的阶数(1 < 3)。在经历了早期的下降之后，进一步减少该模型的训练损失变得困难。在最后一个迭代周期完成后，*训练损失*仍然很高。
* 当用来拟合非线性模式（如这里的三阶多项式函数）时，线性模型容易*欠拟合*。


```python
# 从多项式特征中选择前2个维度，即 1, x
train(poly_features[:n_train, :2], poly_features[n_train:, :2],
      labels[:n_train], labels[n_train:])
```

    weight: [[3.163933 4.417879]]




![output_24_1](/Users/zhangjie/Downloads/模型选择、欠拟合和过拟合/output_24_1.svg)
    


###  高阶多项式函数拟合(过拟合)

* 现在，让我们尝试使用一个阶数过高的多项式来训练模型。在这种情况下，没有足够的数据用于学到*高阶系数应该具有接近于零的值*。因此，这个过于复杂的模型会轻易受到训练数据中噪声的影响。虽然训练损失可以有效地降低，但*测试损失仍然很高*。
* 结果表明，复杂模型对数据造成了过拟合。


```python
# 从多项式特征中选取所有维度，即20个维度全选（也会有20个权重参数）
train(poly_features[:n_train, :], poly_features[n_train:, :],
      labels[:n_train], labels[n_train:], num_epochs=1500)
```

    weight: [[ 4.996112    1.3451424  -3.4369674   4.9781423   0.24880387  1.3470473
      -0.26855323 -0.33285204  0.41843835 -0.3727278  -0.1663124   0.23328507
       0.31306687  0.3988585   0.34989536  0.06694403  0.27965528  0.23832247
       0.1961437   0.16942483]]




![output_26_1](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/output_26_1.svg)
    

# 权重衰减

* 通过去收集更多的训练数据来缓解过拟合。但这可能成本很高而且耗时.或者完全超出我们的控制，在短期内不可能做到。
* 我们通常通过*正则化模型技术*去缓解过拟合问题。权重衰减是最广泛使用的正则化的技术之一。
* 在多项式回归的例子中，我们可以通过调整拟合多项式的阶数来限制模型的容量。实际上，限制特征的数量是缓解过拟合的一种常用技术。然而，简单地丢弃特征对于这项工作来说可能过于生硬。即使是阶数上的微小变化，比如从$2$到$3$，也会显著增加我们模型的复杂性。因此，我们经常需要一个更细粒度的工具来调整函数的复杂性。

## 范数与权重衰减

[深度学习中的数学-范数](https://www.zestaken.top/post/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%89%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6)
* 在训练参数化机器学习模型时，*权重衰减*（通常称为$L_2$正则化）是最广泛使用的正则化的技术之一。这项技术是基于一个基本直觉，即在所有函数$f$中，函数$f = 0$（所有输入都得到值$0$）在某种意义上是最简单的，我们可以通过函数与零的距离来衡量函数的复杂度。
* 度量函数与零的距离方法：是通过线性函数$f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x}$中的权重向量的某个范数来度量其复杂性，例如$\| \mathbf{w} \|^2$。
    * 要保证权重向量比较小，最常用方法是将其*范数作为惩罚项加到最小化损失的问题中*。将原来的训练目标*最小化训练标签上的预测损失*，调整为*最小化预测损失和惩罚项之和*。
    * 现在，如果我们的权重向量增长的太大，我们的学习算法可能会更集中于最小化权重范数$\| \mathbf{w} \|^2$。这正是我们想要的。
* 线性回归例子。我们的损失由下式给出：

    $$L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.$$

    * $\mathbf{x}^{(i)}$是样本$i$的特征，$y^{(i)}$是样本$i$的标签。$(\mathbf{w}, b)$是权重和偏置参数。
    * 为了惩罚权重向量的大小，我们必须以某种方式在损失函数中添加$\| \mathbf{w} \|^2$，但是模型应该如何平衡这个新的额外惩罚的损失？实际上，我们通过*正则化常数*$\lambda$来描述这种权衡，这是一个非负超参数，我们使用验证数据拟合：

    $$L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2,$$

    * 对于$\lambda = 0$，我们恢复了原来的损失函数。对于$\lambda > 0$，我们限制$\| \mathbf{w} \|$的大小。我们仍然除以$2$：当我们取一个二次函数的导数时，$2$和$1/2$会抵消，以确保更新表达式看起来既漂亮又简单。
* 使用$L_2$范数的$L_2$正则化线性模型构成经典的*岭回归*（ridge regression）算法，使用$L_1$范数的$L_1$正则化线性回归是统计学中类似的基本模型，通常被称为*套索回归*（lasso regression）。
    * 使用$L_2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。这使得我们的学习算法*偏向于在大量特征上均匀分布权重的模型*。在实践中，这可能使它们对单个变量中的观测误差更为鲁棒。
    * 相比之下，$L_1$惩罚会导致模型将*其他权重清除为零而将权重集中在一小部分特征上*。这称为*特征选择*（feature selection），这可能是其他场景下需要的。
* $L_2$正则化回归的小批量随机梯度下降更新如下式：

    $$
    \begin{aligned}
    \mathbf{w} & \leftarrow \left(1- \eta\lambda \right) \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right).
    \end{aligned}
    $$
* 我们根据估计值与观测值之间的差异来更新$\mathbf{w}$。然而，我们同时也在试图将$\mathbf{w}$的大小缩小到零。这就是为什么这种方法有时被称为*权重衰减*。我们仅考虑惩罚项，优化算法在训练的每一步*衰减*权重。与特征选择相比，权重衰减为我们提供了一种连续的机制来调整函数的复杂度。较小的$\lambda$值对应较少约束的$\mathbf{w}$，而较大的$\lambda$值对$\mathbf{w}$的约束更大。
* 是否对相应的偏置$b^2$进行惩罚在不同的实现中会有所不同。在神经网络的不同层中也会有所不同。通常，我们不正则化网络输出层的偏置项。

## 高维线性回归

我们通过一个简单的例子来说明演示权重衰减。



```python
%matplotlib inline
import tensorflow as tf
from d2l import tensorflow as d2l
```

* 首先，我们[**像以前一样生成一些数据**]，生成公式如下：

    $$y = 0.05 + \sum_{i = 1}^d 0.01 x_i + \epsilon \text{ where }
\epsilon \sim \mathcal{N}(0, 0.01^2).$$

* 我们选择标签是关于输入的线性函数。标签同时被均值为0，标准差为0.01高斯噪声破坏。为了使过拟合的效果更加明显，我们可以将问题的维数增加到$d = 200$，并使用一个只包含20个样本的小训练集。synthetic_data和load_array函数见[自定义函数之生成数据集函数](https://www.zestaken.top/post/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0)


```python
n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5 # 数据大小和训练批量
true_w, true_b = tf.ones((num_inputs, 1)) * 0.01, 0.05 # 创建参数矩阵
train_data = d2l.synthetic_data(true_w, true_b, n_train) # 生成 y = Xw + b + 噪声形的数据
train_iter = d2l.load_array(train_data, batch_size) # 构造数据迭代器函数
test_data = d2l.synthetic_data(true_w, true_b, n_test)
test_iter = d2l.load_array(test_data, batch_size, is_train=False)
```

## 从零开始实现

* 在下面，我们将从头开始实现权重衰减，只需将$L_2$的平方惩罚添加到原始目标函数中。

### 初始化模型参数

* 首先，我们将定义一个函数来*随机初始化我们的模型参数*。


```python
def init_params():
    w = tf.Variable(tf.random.normal(mean=1, shape=(num_inputs, 1)))
    b = tf.Variable(tf.zeros(shape=(1, )))
    return [w, b]
```

### 定义$L_2$范数惩罚

* 实现这一惩罚最方便的方法是对所有项求平方后并将它们求和。


```python
def l2_penalty(w):
    return tf.reduce_sum(tf.pow(w, 2)) / 2 # 平方再求和再乘以二分之一,仅依赖当前参数值计算
```

### 定义训练代码实现

* 下面的代码将模型拟合训练数据集，并在测试数据集上进行评估。线性网络和平方损失没有变化，所以我们通过`d2l.linreg`和`d2l.squared_loss`导入它们[自定义函数之模型、损失函数和优化算法中函数](https://www.zestaken.top/post/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0)。唯一的变化是损失现在包括了惩罚项。[lambda函数使用](https://www.w3school.com.cn/python/python_lambda.asp)


```python
def train(lambd):
    w, b = init_params()
    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss
    num_epochs, lr = 100, 0.003
    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
                            xlim=[5, num_epochs], legend=['train', 'test'])
    for epoch in range(num_epochs):
        for X, y in train_iter:
            with tf.GradientTape() as tape:
                # 增加了L2范数惩罚项，广播机制使l2_penalty(w)成为一个长度为`batch_size`的向量。
                l = loss(net(X), y) + lambd * l2_penalty(w) # 给损失加上了惩罚项
            grads = tape.gradient(l, [w, b])
            d2l.sgd([w, b], grads, lr, batch_size)
        if (epoch + 1) % 5 == 0:
            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),
                                     d2l.evaluate_loss(net, test_iter, loss)))
    print('w的L2范数是：', tf.norm(w).numpy())
```

### 忽略正则化直接训练

* 我们现在用`lambd = 0`禁用权重衰减后运行这个代码。注意，这里训练误差有了减少，但测试误差没有减少。这意味着出现了严重的过拟合。这是过拟合的一个典型例子。


```python
train(lambd=0)
```

    w的L2范数是： 17.892567




![output_13_1](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/output_13_1.svg))
    


### 使用权重衰减

* 下面，我们使用权重衰减来运行代码。注意，在这里训练误差增大，但测试误差减小。这正是我们期望从正则化中得到的效果。


```python
train(lambd=3)
```

    w的L2范数是： 0.5354594




![output_15_1](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/output_15_1.svg)
    


## 简洁实现

* 由于权重衰减在神经网络优化中很常用，深度学习框架为了便于使用权重衰减，便将权重衰减集成到优化算法中，以便与任何损失函数结合使用。此外，这种集成还有计算上的好处，允许在不增加任何额外的计算开销的情况下向算法中添加权重衰减。由于更新的权重衰减部分仅依赖于每个参数的当前值，因此优化器必须至少接触每个参数一次。

* 在下面的代码中，我们使用权重衰减超参数`wd`（即$\lambda$参数）创建一个$L_2$正则化器，并通过`kernel_regularizer`参数将其应用于网络层。


```python
def train_concise(wd):
    net = tf.keras.models.Sequential()
    net.add(tf.keras.layers.Dense(
        1, kernel_regularizer=tf.keras.regularizers.l2(wd))) # 通过kernel_regularizer参数添加L2范数
    net.build(input_shape=(1, num_inputs))
    w, b = net.trainable_variables
    loss = tf.keras.losses.MeanSquaredError()
    num_epochs, lr = 100, 0.003
    trainer = tf.keras.optimizers.SGD(learning_rate=lr)
    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
                            xlim=[5, num_epochs], legend=['train', 'test'])
    for epoch in range(num_epochs):
        for X, y in train_iter:
            with tf.GradientTape() as tape:
                # `tf.keras` 需要为自定义训练代码手动添加损失。
                l = loss(net(X), y) + net.losses # net.losses就是框架生成的权重衰减范数
            grads = tape.gradient(l, net.trainable_variables)
            trainer.apply_gradients(zip(grads, net.trainable_variables))
        if (epoch + 1) % 5 == 0:
            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),
                                     d2l.evaluate_loss(net, test_iter, loss)))
    print('w的L2范数：', tf.norm(net.get_weights()[0]).numpy())
```

* 这些图看起来和我们从零开始实现权重衰减时的图相同。然而，它们运行得更快，更容易实现，对于更复杂的问题，这一好处将变得更加明显。


```python
train_concise(0)# 可以看出wd参数就是从零实现中的lambd参数
```

    w的L2范数： 1.3891902




![output_20_1](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/output_20_1.svg)
    



```python
train_concise(3)
```

    w的L2范数： 0.039072886




![output_21_1](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/output_21_1.svg)
    

# Dropout

## Dropout的概念

* 对于一个好的预测模型，我们期待它能在看不见的数据上有很好的表现。经典泛化理论认为，为了缩小训练和测试性能之间的差距，我们应该以简单的模型为目标。
    * 简单性以较小维度的形式出现。
    * 此外，正如我们在讨论权重衰减（$L_2$正则化）时看到的那样，参数的范数也代表了一种有用的简单性度量。
    * 简单性的另一个有用角度是平滑性，即函数不应该对其输入的微小变化敏感。例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。
* 1995年，克里斯托弗·毕晓普证明了具有输入噪声的训练等价于Tikhonov正则化 ，从而将这平滑性一观点正式化。这项工作在要求函数光滑(因而简单)和要求它对输入中的扰动具有适应性之间有了明确的数学联系。
    * 在2014年，斯里瓦斯塔瓦等人还就如何将毕晓普的想法应用于网络的内部层提出了一个聪明的想法：在训练过程中，他们建议在计算后续层之前向网络的*每一层注入噪声*。他们意识到，当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。
* **Dropout**：斯里瓦斯塔瓦等人的想法被称为*暂退法*（dropout），dropout在正向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的标准技术。这种方法之所以被称为 *dropout* ，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。在整个训练过程的每一次迭代中，dropout包括在计算下一层之前将当前层中的一些节点置零。
* 如何注入噪声：在标准dropout正则化中，通过按保留（未丢弃）的节点的分数进行归一化来消除每一层的偏差。换言之，每个中间激活值$h$以*丢弃概率*$p$由随机变量$h'$替换，如下所示：
    $$
    \begin{aligned}
    h' =
    \begin{cases}
        0 & \text{ 概率为 } p \\
        \frac{h}{1-p} & \text{ 其他情况}
    \end{cases}
    \end{aligned}
    $$
    * 根据设计，期望值保持不变，即$E[h'] = h$。

## 实践中的dropout

* 例如：带有一个隐藏层和5个隐藏单元的多层感知机。当我们将dropout应用到隐藏层，以$p$的概率将隐藏单元置为零时，结果可以看作是一个只包含原始神经元子集的网络。在下图中，删除了$h_2$和$h_5$。因此，输出的计算不再依赖于$h_2$或$h_5$，并且它们各自的梯度在执行反向传播时也会消失。这样，输出层的计算不能过度依赖于$h_1, \ldots, h_5$的任何一个元素。
    ![2G9ShM](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/2G9ShM.png)
* 通常，我们*在测试时禁用dropout*。给定一个训练好的模型和一个新的样本，我们不会丢弃任何节点，因此不需要标准化。然而，也有一些例外：一些研究人员使用测试时的dropout作为估计神经网络预测的“不确定性”的启发式方法：如果预测在许多不同的dropout掩码上都是一致的，那么我们可以说网络更有自信心。

## 从零开始实现

* 要实现单层的dropout函数，我们必须从伯努利（二元）随机变量中提取与我们的层的维度一样多的样本，其中随机变量以概率$1-p$取值$1$（保持），以概率$p$取值$0$（丢弃）。实现这一点的一种简单方式是首先从均匀分布$U[0, 1]$中抽取样本。那么我们可以保留那些对应样本大于$p$的节点，把剩下的丢弃。
* 在下面的代码中，**我们实现 `dropout_layer` 函数，该函数以`dropout`的概率丢弃张量输入`X`中的元素**，如上所述重新缩放剩余部分：将剩余部分除以`1.0-dropout`。


```python
import tensorflow as tf
from d2l import tensorflow as d2l


def dropout_layer(X, dropout):
    assert 0 <= dropout <= 1
    # 在本情况中，所有元素都被丢弃。
    if dropout == 1:
        return tf.zeros_like(X)
    # 在本情况中，所有元素都被保留。
    if dropout == 0:
        return X
    # mask是布尔数组
    mask = tf.random.uniform(
        shape=tf.shape(X), minval=0, maxval=1) < 1 - dropout # 从均匀分布中获取数值，小于1-dropout的为false丢弃
    return tf.cast(mask, dtype=tf.float32) * X / (1.0 - dropout)# 
```

* 我们可以通过几个例子来[**测试`dropout_layer`函数**]。在下面的代码行中，我们将输入`X`通过dropout操作，*丢弃概率*分别为0、0.5和1。


```python
X = tf.reshape(tf.range(16, dtype=tf.float32), (2, 8))
print(X)
print(dropout_layer(X, 0.)) # 丢弃概率为0时，一个都不丢弃，张量值不变
print(dropout_layer(X, 0.5))
print(dropout_layer(X, 1.)) #丢弃概率为1时，全部丢弃，张量值全为0
```

    tf.Tensor(
    [[ 0.  1.  2.  3.  4.  5.  6.  7.]
     [ 8.  9. 10. 11. 12. 13. 14. 15.]], shape=(2, 8), dtype=float32)
    tf.Tensor(
    [[ 0.  1.  2.  3.  4.  5.  6.  7.]
     [ 8.  9. 10. 11. 12. 13. 14. 15.]], shape=(2, 8), dtype=float32)
    tf.Tensor(
    [[ 0.  0.  0.  0.  8. 10.  0. 14.]
     [16. 18.  0.  0.  0.  0. 28. 30.]], shape=(2, 8), dtype=float32)
    tf.Tensor(
    [[0. 0. 0. 0. 0. 0. 0. 0.]
     [0. 0. 0. 0. 0. 0. 0. 0.]], shape=(2, 8), dtype=float32)


### 定义模型参数

* 我们使用Fashion-MNIST数据集。我们[**定义具有两个隐藏层的多层感知机，每个隐藏层包含256个单元**]。


```python
num_outputs, num_hiddens1, num_hiddens2 = 10, 256, 256
```

### 定义模型

* 下面的模型将dropout应用于每个隐藏层的输出（在激活函数之后）。我们可以分别为每一层设置丢弃概率。
* 一种常见的技巧是在靠近输入层的地方设置较低的丢弃概率。
* 下面，我们将第一个和第二个隐藏层的丢弃概率分别设置为0.2和0.5。我们确保dropout只在训练期间有效。


```python
dropout1, dropout2 = 0.2, 0.5 # 丢弃概率

class Net(tf.keras.Model):
    def __init__(self, num_outputs, num_hiddens1, num_hiddens2):
        super().__init__()
        self.input_layer = tf.keras.layers.Flatten()
        self.hidden1 = tf.keras.layers.Dense(num_hiddens1, activation='relu') # 隐藏层
        self.hidden2 = tf.keras.layers.Dense(num_hiddens2, activation='relu')
        self.output_layer = tf.keras.layers.Dense(num_outputs) # 全连接层

    def call(self, inputs, training=None):
        x = self.input_layer(inputs)
        x = self.hidden1(x)
        # 只有在训练模型时才使用dropout
        if training:
            # 在第一个全连接层之后添加一个dropout层
            x = dropout_layer(x, dropout1)
        x = self.hidden2(x)
        if training:
            # 在第二个全连接层之后添加一个dropout层
            x = dropout_layer(x, dropout2)
        x = self.output_layer(x)
        return x

net = Net(num_outputs, num_hiddens1, num_hiddens2) # 生成模型
```

###  训练和测试

* 这类似于前面描述的多层感知机训练和测试。`load_data_fashion_mnist`和`train_ch3`函数定义见[自定义函数之生成数据集函数和训练函数](https://www.zestaken.top/post/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0)


```python
num_epochs, lr, batch_size = 10, 0.5, 256 
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # 损失函数
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) # 生成数据迭代器
trainer = tf.keras.optimizers.SGD(learning_rate=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```


![output_12_0](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/output_12_0.svg)
    


## 简洁实现

* 对于高级API，我们所需要做的就是在每个全连接层之后添加一个`Dropout`层，将丢弃概率作为唯一的参数传递给它的构造函数。在训练过程中，`Dropout`层将根据指定的丢弃概率随机丢弃上一层的输出（相当于下一层的输入）。当不处于训练模式时，`Dropout`层仅在测试时传递数据。


```python
net = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256, activation=tf.nn.relu),
    # 在第一个全连接层之后添加一个dropout层
    tf.keras.layers.Dropout(dropout1),
    tf.keras.layers.Dense(256, activation=tf.nn.relu),
    # 在第二个全连接层之后添加一个dropout层
    tf.keras.layers.Dropout(dropout2),
    tf.keras.layers.Dense(10),
])
```

* 接下来，我们[**对模型进行训练和测试**]。结果是一致的


```python
trainer = tf.keras.optimizers.SGD(learning_rate=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```


​    
![output_16_0](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/output_16_0.svg)
​    

