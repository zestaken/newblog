---
title: 深度学习十三——卷积神经网络
date: 2021-12-06 22:52:19
tags: [深度学习]
categories: 技术笔记
---
# 循环神经网络概述

* 卷积神经网络可以有效地处理空间信息，*循环神经网络（recurrent neural network, RNN）*这种设计可以更好地处理*序列信息*。
* 循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。

# 序列模型

* 序列模型应用于处理前后序列之间会有影响的数据，如：
    * 音乐、语音、文本和视频都是连续的。如果它们的序列被我们重排，那么原有的意义就会失去。
    * 地震具有很强的相关性，即大地震发生后，很可能会有几次较小的余震，这些余震的强度比不是大地震的余震要大得多。

## 统计工具

* 处理序列数据需要统计工具和新的深度神经网络结构。为了简单起见，我们以下图所示的股票价格为例：
    ![](https://zjpicture.oss-cn-beijing.aliyuncs.com/img/20211201220224.png)
    * 其中，用 $x_t$ 表示价格，即在 *时间步*（time step）$t \in \mathbb{Z}^+$时，观察到的价格 $x_t$。
    * 请注意，$t$ 对于本文中的序列通常是离散的，并随整数或其子集而变化。假设一个交易员想在 $t$ 日的股市中表现良好，于是通过以下途径预测 $x_t$：
    $$x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1).$$

### 自回归模型

* 为了实现这个预测，交易员可以使用回归模型，仅有一个主要问题：输入数据的数量，输入 $x_{t-1}, \ldots, x_1$ 本身因 $t$ 而异。也就是说，输入数据的数量这个数字将会随着我们遇到的数据量的增加而增加，因此需要一个近似方法来使这个计算变得容易处理。即如何有效估计 $P(x_t \mid x_{t-1}, \ldots, x_1)$ 。简单地说，它归结为以下两种策略：
    *  自回归模型：假设在现实情况下相当长的序列 $x_{t-1}, \ldots, x_1$ 可能是不必要的，因此我们只需要满足某个长度为 $\tau$ 的时间跨度，即使用观测序列 $x_{t-1},\ldots, x_{t-\tau}$。当下获得的最直接的好处就是*参数的数量总是不变*的，至少在 $t > \tau$ 时如此，这就使我们能够训练一个上面提及的深度网络。这种模型被称为 *自回归模型*（autoregressive models），因为它们就是对自己执行回归。
    * 隐变量自回归模型：如下图所示，是保留一些对过去观测的总结 $h_t$，并且同时更新预测 $\hat{x}_t$ 和总结 $h_t$。这就产生了基于 $\hat{x}_t = P(x_t \mid h_{t})$ 估计 $x_t$，以及公式 $h_t = g(h_{t-1}, x_{t-1})$ 更新的模型。由于 $h_t$ 从未被观测到，这类模型也被称为 *隐变量自回归模型*（latent autoregressive models）。
     ![](https://zjpicture.oss-cn-beijing.aliyuncs.com/img/20211201220911.png)
* 这两种情况都有一个显而易见的问题，即如何生成训练数据：
    * 一个经典方法是使用*到目前为止的历史观测来预测下一个未来观测*。
    * 一个常见的假设是虽然特定值 $x_t$ 可能会改变，但是序列本身前后影响的关系不会改变。这样的假设是合理的。统计学家称这种前后影响关系不变的序列为 *静止的*（stationary）。因此，无论我们做什么，整个序列的估计值都将通过以下的方式获得
    $$P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}, \ldots, x_1).$$
* 如果我们处理的是离散的对象（如单词），而不是连续的数字，则上述的考虑仍然有效。唯一的差别是，对于离散的对象，我们需要使用分类器而不是回归模型来估计$P(x_t \mid  x_{t-1}, \ldots, x_1)$。

### 马尔可夫模型

* 马尔可夫条件：在自回归模型的近似法中，我们使用 $x_{t-1}, \ldots, x_{t-\tau}$ 而不是 $x_{t-1}, \ldots, x_1$ 来估计 $x_t$。只要这种近似是精确的，我们就说序列满足 *马尔可夫条件*（Markov condition）。特别是，如果 $\tau = 1$，得到一个 *一阶马尔可夫模型*（first-order Markov model），$P(x)$ 由下式给出：
    $$P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}) \text{ where } P(x_1 \mid x_0) = P(x_1).$$
* 当假设 $x_t$ 仅是离散值时，这样的模型特别棒，因为在这种情况下，使用动态规划可以沿着马尔可夫链精确地计算结果。例如，我们可以高效地计算$P(x_{t+1} \mid x_{t-1})$：
    $$
    \begin{aligned}
    P(x_{t+1} \mid x_{t-1})
    &= \frac{\sum_{x_t} P(x_{t+1}, x_t, x_{t-1})}{P(x_{t-1})}\\
    &= \frac{\sum_{x_t} P(x_{t+1} \mid x_t, x_{t-1}) P(x_t, x_{t-1})}{P(x_{t-1})}\\
    &= \sum_{x_t} P(x_{t+1} \mid x_t) P(x_t \mid x_{t-1})
    \end{aligned}
    $$
* 利用这一事实，我们只需要考虑过去观察中的一个非常短的历史：$P(x_{t+1} \mid x_t, x_{t-1}) = P(x_{t+1} \mid x_t)$。

### 因果关系

* 原则上，将 $P(x_1, \ldots, x_T)$ 倒序展开也没啥问题。毕竟，基于条件概率公式，我们总是可以写出：
    $$P(x_1, \ldots, x_T) = \prod_{t=T}^1 P(x_t \mid x_{t+1}, \ldots, x_T).$$
* 事实上，如果基于一个马尔可夫模型，我们还可以得到一个反向的条件概率分布。这样方向的实质就是根据未来的事情推测前面已经发生的事情。然而，在许多情况下，数据存在一个自然的方向，即在时间上是前进的。很明显，未来的事件不能影响过去。因此，如果我们改变 $x_t$，可能会影响未来发生的事情 $x_{t+1}$，但不能反过来。即，事件发生的因果关系不能颠倒。

## 训练

* 下面，让我们在实践中尝试一下前面所说的模型。
* 首先，生成一些数据。简单起见，我们(**使用正弦函数和一些可加性噪声来生成序列数据，时间步为 $1, 2, \ldots, 1000$。**)


```python
%matplotlib inline
import tensorflow as tf
from d2l import tensorflow as d2l
```


```python
T = 1000  # 总共产生1000个点
time = tf.range(1, T + 1, dtype=d2l.float32) # 产生从1， 2到1000的时间步
x = tf.sin(0.01 * time) + tf.random.normal([T], 0, 0.2) # 使用正弦函数加随机啊噪声根据时间步生成数据
d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3)) # 绘制出数据与时间函数图像
```


![](https://zjpicture.oss-cn-beijing.aliyuncs.com/img/20211206224400.svg)
    


* 接下来，我们将这样的序列转换为模型可以训练的*特征和标签*。
    * 基于嵌入维度 $\tau$，我们[**将数据映射为数据对 $y_t = x_t$ 和 $\mathbf{x}_t = [x_{t-\tau}, \ldots, x_{t-1}]$。**]
    * 但是这比我们提供的数据样本少了 $\tau$ 个，因为我们没有足够的历史记录来描述前 $\tau$ 个数据样本。
    * 一个简单的解决办法，尤其是如果拥有足够长的序列就丢弃这几项；另一个方法，我们可以用零填充序列。在这里，我们仅使用前600个“特征－标签”（feature-label）对进行训练。
    * 简而言之，对于下面代码来说，特征变量分别为从(0, 996)(1, 997)(2, 998)(3, 999),而标签变量为(4, 996)。即一行（3+1）是一次训练的数据，输入是前三个变量，而输出是第4个变量，通过计算模型计算的第4个变量的值愈标签向量里存储的第4个变量的差值来优化模型。


```python
tau = 4
# 舍弃掉前4个变量和最后4个变量
features = tf.Variable(tf.zeros((T - tau, tau))) # 初始化一个（996， 4）的全零矩阵，并转换为tensor张量
for i in range(tau): 
    features[:, i].assign(x[i: T - tau + i]) # 从变量中取出996个放入features矩阵的一列中，
labels = tf.reshape(x[tau:], (-1, 1)) # 将变量的后996个值作为标签，生成一个(996, 1)的标签矩阵
```


```python
batch_size, n_train = 16, 600
# 只有前`n_train`个样本用于训练
train_iter = d2l.load_array((features[:n_train], labels[:n_train]), # 使用特征矩阵和标签矩阵的前600个样本训练模型
                            batch_size, is_train=True)
```

* 训练模型[**使用一个相当简单的结构：只是一个拥有两个全连接层的多层感知机**]，ReLU激活函数和平方损失。
* 注：[训练相关函数定义](https://www.zestaken.top/post/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0)


```python
# 普通的多层感知机模型
def get_net():
    net = tf.keras.Sequential([tf.keras.layers.Dense(10, activation='relu'),
                              tf.keras.layers.Dense(1)]) # 构造两个全连接层的多层感知机
    return net

# 最小均方损失
loss = tf.keras.losses.MeanSquaredError()
```

* 下面训练模型：


```python
def train(net, train_iter, loss, epochs, lr):
    trainer = tf.keras.optimizers.Adam() # Adam优化器
    for epoch in range(epochs):
        for X, y in train_iter:
            with tf.GradientTape() as g: # 计算损失与优化更新参数
                out = net(X)
                l = loss(y, out) / 2
                params = net.trainable_variables
                grads = g.gradient(l, params)
            trainer.apply_gradients(zip(grads, params))
        print(f'epoch {epoch + 1}, '
              f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}') # 打印每一次迭代的平均损失

net = get_net() # 获取模型
train(net, train_iter, loss, 5, 0.01) # 进行训练
```

    epoch 1, loss: 0.746276
    epoch 2, loss: 0.365285
    epoch 3, loss: 0.197580
    epoch 4, loss: 0.131737
    epoch 5, loss: 0.093785


## 预测

* 由于训练损失很小，因此我们期望模型能有很好的工作效果。让我们看看这在实践中意味着什么。
* 首先是检查[**模型预测下一个时间步**]发生的是什么的能力，也就是 *单步预测*（one-step-ahead prediction）。


```python
onestep_preds = net(features)  # 使用模型进行单步预测，即输入一个3个特征输出第4个特征
d2l.plot([time, time[tau:]], [x.numpy(), onestep_preds.numpy()], 'time',
         'x', legend=['data', '1-step preds'], xlim=[1, 1000], figsize=(6, 3))
```


![](https://zjpicture.oss-cn-beijing.aliyuncs.com/img/20211206224432.svg)
    


* 单步预测效果不错。即使这些预测的时间步超过了 $600+4$（`n_train + tau`），其结果看起来仍然是可信的。然而有一个小问题：如果数据观察序列的时间步只到 $604$，那么我们就没办法指望能够得到所有未来的单步预测作为输入。相反，我们需要一步一步地向前迈进：
    $$
    \hat{x}_{605} = f(x_{601}, x_{602}, x_{603}, x_{604}), \\
    \hat{x}_{606} = f(x_{602}, x_{603}, x_{604}, \hat{x}_{605}), \\
    \hat{x}_{607} = f(x_{603}, x_{604}, \hat{x}_{605}, \hat{x}_{606}),\\
    \hat{x}_{608} = f(x_{604}, \hat{x}_{605}, \hat{x}_{606}, \hat{x}_{607}),\\
    \hat{x}_{609} = f(\hat{x}_{605}, \hat{x}_{606}, \hat{x}_{607}, \hat{x}_{608}),\\
    \ldots
    $$
* 通常，对于直到 $x_t$ 的观测序列，其在时间步 $t+k$ 处的预测输出 $\hat{x}_{t+k}$ 称为 *$k$ 步预测*（$k$-step-ahead-prediction）。由于我们的观察已经到了 $x_{604}$，它的 $k$ 步预测是 $\hat{x}_{604+k}$。换句话说，我们必须使用我们自己的预测（而不是原始数据）来[**进行多步预测**]。让我们看看效果如何。


```python
multistep_preds = tf.Variable(tf.zeros(T)) # 创建多步预测数据张量并用0初始化
multistep_preds[:n_train + tau].assign(x[:n_train + tau]) # 将多步预测数据张量用变量的对等位置的值初始化
for i in range(n_train + tau, T): # 将多步变量未赋值位置的值用模型训练出来的值填充
    multistep_preds[i].assign(tf.reshape(net(
        tf.reshape(multistep_preds[i - tau: i], (1, -1))), ())) # 模型根据输入的当前位置前tau个特征值计算出当前位置的值
```


```python
d2l.plot([time, time[tau:], time[n_train + tau:]],
         [x.numpy(), onestep_preds.numpy(),
          multistep_preds[n_train + tau:].numpy()], 'time',
         'x', legend=['data', '1-step preds', 'multistep preds'],
         xlim=[1, 1000], figsize=(6, 3))
```


![](https://zjpicture.oss-cn-beijing.aliyuncs.com/img/20211206224447.svg)
    


* 如上面的例子所示，这是一个巨大的失败。经过几个预测步骤之后，预测的结果很快就会衰减到一个常数。
    * 为什么这个算法效果这么差呢？最终事实是由于错误的累积。
    * 假设在步骤 $1$ 之后，我们积累了一些错误 $\epsilon_1 = \bar\epsilon$。于是，步骤 $2$ 的 *输入*（input）被扰动了 $\epsilon_1$，结果积累的误差是依照次序的 $\epsilon_2 = \bar\epsilon + c \epsilon_1$，其中 $c$ 为某个常数，后面的预测误差依此类推。因此误差可能会相当快地偏离真实的观测结果。例如，未来 $24$ 小时的天气预报往往相当准确，但超过这一点，准确率就会迅速下降。
* 基于 $k = 1, 4, 16, 64$，通过对整个序列预测的计算，让我们更仔细地看一下$k$步预测的困难。


```python
max_steps = 64
```


```python
features = tf.Variable(tf.zeros((T - tau - max_steps + 1, tau + max_steps)))
# 列 `i` (`i` < `tau`) 是来自 `x` 的观测
# 其时间步从 `i + 1` 到 `i + T - tau - max_steps + 1`
for i in range(tau):
    features[:, i].assign(x[i: i + T - tau - max_steps + 1].numpy())

# 列 `i` (`i` >= `tau`) 是 (`i - tau + 1`)步预测
# 其时间步从 `i + 1` 到 `i + T - tau - max_steps + 1`
for i in range(tau, tau + max_steps):
    features[:, i].assign(tf.reshape(net((features[:, i - tau: i])), -1))
```


```python
steps = (1, 4, 16, 64)
d2l.plot([time[tau + i - 1: T - max_steps + i] for i in steps],
         [features[:, (tau + i - 1)].numpy() for i in steps], 'time', 'x',
         legend=[f'{i}-step preds' for i in steps], xlim=[5, 1000],
         figsize=(6, 3))
```


![](https://zjpicture.oss-cn-beijing.aliyuncs.com/img/20211206225326.svg)
    

* 这清楚地说明了当我们试图预测更远的未来时，预测的质量是如何变化的。虽然“$4$ 步预测”看起来仍然不错，但超过这个跨度的任何预测几乎都是无用的。

# 文本预处理

* 序列数据存在许多种形式，文本是最常见例子之一。例如，一篇文章可以简单地看作是一串单词序列，甚至是一串字符序列。
* 下面，我们将学习文本的如下常见预处理步骤：
    1. 将文本作为字符串加载到内存中。
    2. 将字符串拆分为词元（如单词和字符）。
    3. 建立一个词汇表，将拆分的词元映射到数字索引。
    4. 将文本转换为数字索引序列，方便模型操作。


```python
import collections
import re
from d2l import tensorflow as d2l
```

## 读取数据集

* 首先，我们从 H.G.Well 的[时光机器](http://www.gutenberg.org/ebooks/35)一书中加载文本。
    * 这是一个相当小的语料库，只有30000多个单词，但足够我们小试牛刀，而现实中的文档集合可能会包含数十亿个单词。
* 下面的函数(**将数据集读取到由多条文本行组成的列表中**)，其中每条文本行都是一个字符串。为简单起见，我们在这里忽略了标点符号和字母大写。


```python
#@save
d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',
                                '090b5e7e70c295757f55df93cb0a180b9691891a')

def read_time_machine():  #@save
    """Load the time machine dataset into a list of text lines."""
    with open(d2l.download('time_machine'), 'r') as f:
        lines = f.readlines()
    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]

lines = read_time_machine()
print(f'# text lines: {len(lines)}')
print(lines[0])
print(lines[10])
```

    Downloading ../data/timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...
    # text lines: 3221
    the time machine by h g wells
    twinkled and his usually pale face was flushed and animated the


## 词元化

* 下面的 `tokenize` 函数将文本行列表作为输入，列表中的每个元素是一个文本序列（如一条文本行）。
    * [**每个文本序列又被拆分成一个词元列表**]，*词元*（token）是文本的基本单位。
    * 最后，返回一个每一行都是*由词元列表组成的列表*，其中的每个词元都是一个字符串（string）。


```python
def tokenize(lines, token='word'):  #@save
    """将文本行拆分为单词或字符词元。默认拆为单词类型"""
    if token == 'word':
        return [line.split() for line in lines]
    elif token == 'char':
        return [list(line) for line in lines]
    else:
        print('错误：未知词元类型：' + token)

tokens = tokenize(lines)
for i in range(11):
    print(tokens[i])
```

    ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']
    []
    []
    []
    []
    ['i']
    []
    []
    ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']
    ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']
    ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']


## 词汇表

* 词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。
* 所以我们[**构建一个字典，通常也叫做*词汇表*（vocabulary），用来将字符串类型的词元映射到从$0$开始的数字索引中**]。
    * 我们先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计，得到的统计结果称之为*语料*（corpus）。
    * 然后根据每个唯一词元的出现*频率*，为其分配一个数字索引。
    * 很少出现的词元通常被*移除*，这可以降低复杂性。
    * 另外，语料库中*不存在或已删除*的任何词元都将映射到一个特定的未知词元 “&lt;unk&gt;” 。
    * 我们可以选择增加一个列表，用于保存那些被保留的词元，例如：填充词元（“&lt;pad&gt;”）；序列开始词元（“&lt;bos&gt;”）；序列结束词元（“&lt;eos&gt;”）。


```python
class Vocab:  #@save
    """文本词汇表"""
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = [] 
        # 按出现频率排序
        counter = count_corpus(tokens)
        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],
                                  reverse=True)
        # 未知词元的索引为0
        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens
        uniq_tokens += [token for token, freq in self.token_freqs
                        if freq >= min_freq and token not in uniq_tokens]
        self.idx_to_token, self.token_to_idx = [], dict()
        for token in uniq_tokens:
            self.idx_to_token.append(token)
            self.token_to_idx[token] = len(self.idx_to_token) - 1

    def __len__(self):
        return len(self.idx_to_token)

    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
        return [self.__getitem__(token) for token in tokens]

    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]

def count_corpus(tokens):  #@save
    """统计词元的频率。"""
    # 这里的 `tokens` 是 1D 列表或 2D 列表
    if len(tokens) == 0 or isinstance(tokens[0], list):
        # 将词元列表展平成使用词元填充的一个列表
        tokens = [token for line in tokens for token in line]
    return collections.Counter(tokens)
```

* 我们首先使用时光机器数据集作为语料库来[**构建词汇表**]，然后打印前几个高频词元及其索引。


```python
vocab = Vocab(tokens)
print(list(vocab.token_to_idx.items())[:10])
```

    [('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]


* 现在，我们可以(**将每一条文本行转换成一个数字索引列表**)。


```python
for i in [0, 10]:
    print('words:', tokens[i])
    print('indices:', vocab[tokens[i]])
```

    words: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']
    indices: [1, 19, 50, 40, 2183, 2184, 400]
    words: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']
    indices: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]


## 整合所有功能

* 在使用上述函数时，我们[**将所有功能打包到`load_corpus_time_machine`函数中**]，该函数返回 `corpus`（词元索引列表）和 `vocab`（时光机器语料库的词汇表）。
* 我们在这里所做的改变是：
    1. 为了简化后面的训练，我们使用字符（而不是单词）实现文本词元化；
    2. 时光机器数据集中的每个文本行不一定是一个句子或一个段落，还可能是一个单词，因此返回的 `corpus` 仅处理为单个列表，而不是使用多个词元列表构成的一个列表。


```python
def load_corpus_time_machine(max_tokens=-1):  #@save
    """返回时光机器数据集的词元索引列表和词汇表。"""
    lines = read_time_machine()
    tokens = tokenize(lines, 'char')
    vocab = Vocab(tokens)
    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，
    # 所以将所有文本行展平到一个列表中
    corpus = [vocab[token] for line in tokens for token in line]
    if max_tokens > 0:
        corpus = corpus[:max_tokens]
    return corpus, vocab

corpus, vocab = load_corpus_time_machine()
len(corpus), len(vocab) # vocab的长度为28是由26个字母加上空格和未知构成的
```




    (170580, 28)

# 语言模型和数据集

* 在文本预处理过程中，我们将文本数据映射为词元，这些词元可以被视为一系列离散的观测，例如单词或字符。
* 假设长度为 $T$ 的文本序列中的词元依次为 $x_1, x_2, \ldots, x_T$。于是，$x_t$($1 \leq t \leq T$) 可以被认为是文本序列在时间步 $t$ 处的观测或标签。在给定这样的文本序列时，*语言模型*（language model）的目标是估计序列的联合概率（即一个字符序列出现的概率）：
    $$P(x_1, x_2, \ldots, x_T).$$

## 学习语言模型

* 显而易见，我们面对的问题是如何对一个文档，甚至是一个词元序列进行建模。假设在单词级别对文本数据进行词元化，我们可以用序列模型来分析。让我们从基本概率规则开始：
    $$P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T P(x_t  \mid  x_1, \ldots, x_{t-1}).$$
* 例如，包含了四个单词的一个文本序列的概率是：
    $$P(\text{deep}, \text{learning}, \text{is}, \text{fun}) =  P(\text{deep}) P(\text{learning}  \mid  \text{deep}) P(\text{is}  \mid  \text{deep}, \text{learning}) P(\text{fun}  \mid  \text{deep}, \text{learning}, \text{is}).$$
    * 为了计算语言模型，我们需要计算单词的概率和给定前面几个单词后出现某个单词的条件概率。这些概率本质上就是语言模型的参数。
* 这里，我们假设训练数据集是一个大型的文本语料库，比如，所有维基百科的条目，[古登堡计划](https://en.wikipedia.org/wiki/Project_Gutenberg)，以及所有发布在网络上的文本。
* 训练数据集中词的概率可以根据给定词的相对词频来计算。例如，可以将估计值 $\hat{P}(\text{deep})$ 计算为任何以单词“deep”开头的句子的概率。一种稍稍不太精确的方法是统计单词“deep”在数据集中的出现次数，然后将其除以整个语料库中的单词总数。这种方法效果不错，特别是对于频繁出现的单词。接下来，我们可以尝试估计
    $$\hat{P}(\text{learning} \mid \text{deep}) = \frac{n(\text{deep, learning})}{n(\text{deep})},$$
* 其中 $n(x)$ 和 $n(x, x')$ 分别是单个单词和连续单词对的出现次数。
* 不幸的是，由于连续单词对“deep learning”的出现频率要低得多，所以估计这类单词正确的概率要困难得多。特别是，对于一些不常见的单词组合，要想找到足够的出现次数来获得准确的估计可能都不容易。而对于三个或者更多单词的组合，情况会变得更糟。许多合理的三个单词组合可能是存在的，但是在数据集中却找不到。除非我们提供某种解决方案来将这些单词组合指定为非零计数，否则将无法在语言模型中使用它们。如果数据集很小，或者单词非常罕见，那么这类单词出现一次的机会可能都找不到。
* 一种常见的策略是执行某种形式的 *拉普拉斯平滑*（Laplace smoothing），具体方法是在所有计数中添加一个小常量。用 $n$ 表示训练集中的单词总数，用 $m$ 表示唯一单词的数量。此解决方案有助于处理单元素问题，例如通过：
    $$
    \begin{aligned}
        \hat{P}(x) & = \frac{n(x) + \epsilon_1/m}{n + \epsilon_1}, \\
        \hat{P}(x' \mid x) & = \frac{n(x, x') + \epsilon_2 \hat{P}(x')}{n(x) + \epsilon_2}, \\
        \hat{P}(x'' \mid x,x') & = \frac{n(x, x',x'') + \epsilon_3 \hat{P}(x'')}{n(x, x') + \epsilon_3}.
    \end{aligned}
    $$
    * 其中，$\epsilon_1,\epsilon_2$ 和 $\epsilon_3$ 是超参数。以 $\epsilon_1$ 为例：当 $\epsilon_1 = 0$ 时，不应用平滑；当 $\epsilon_1$ 接近正无穷大时，$\hat{P}(x)$ 接近均匀概率分布 $1/m$。
    * 但是这样的模型很容易就会因为下面的原因变得无效：
        * 首先，我们需要存储所有的计数；
        * 其次，这完全忽略了单词的意思。例如，“猫”（cat）和“猫科动物”（feline）应该出现在相关的上下文中，但是想根据额外的上下文调整这些模型其实是相当困难的，然而基于深度学习的语言模型很适合解决这个问题。
        * 最后，长单词序列几乎可以肯定是没见过的，因此一个模型如果只是简单地统计先前看到的单词序列的频率，那么模型面对这种问题肯定是表现不佳的。

## 马尔可夫模型与$n$元语法

* 我们可以将马尔可夫模型，应用于语言建模。如果 $P(x_{t+1} \mid x_t, \ldots, x_1) = P(x_{t+1} \mid x_t)$，则序列上的分布满足一阶马尔可夫性质。阶数越高，对应的依赖关系就越长。这种性质推导出了许多可以应用于序列建模的近似公式：
    $$
    \begin{aligned}
    P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2) P(x_3) P(x_4),\\
    P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_2) P(x_4  \mid  x_3),\\
    P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_1, x_2) P(x_4  \mid  x_2, x_3).
    \end{aligned}
    $$
* 通常，涉及一个、两个和三个变量的概率公式分别被称为“一元语法”（unigram）、“二元语法”（bigram）和“三元语法”（trigram）模型。

## 自然语言统计

* 让我们看看在真实数据上效果如何。根据时光机器数据集构建词汇表，并打印前 $10$ 个最常用的（频率最高的）单词。


```python
import random
import tensorflow as tf
from d2l import tensorflow as d2l
```


```python
tokens = d2l.tokenize(d2l.read_time_machine())
# 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起
corpus = [token for line in tokens for token in line]
vocab = d2l.Vocab(corpus)
vocab.token_freqs[:10]
```




    [('the', 2261),
     ('i', 1267),
     ('and', 1245),
     ('of', 1155),
     ('a', 816),
     ('to', 695),
     ('was', 552),
     ('in', 541),
     ('that', 443),
     ('my', 440)]



* 正如我们所看到的，事实上(**最流行的词**)看起来很无聊。这些词通常(**被称为*停用词***)（stop words），因此可以被过滤掉。尽管如此，它们本身仍然是有意义的，我们仍然会在模型中使用它们。此外，还有个明显的问题是词频衰减的速度相当地快。例如，最常用单词的词频对比，第 $10$ 个还不到第 $1$ 个的 $1/5$。
* 下面我们可以画出词频图。


```python
freqs = [freq for token, freq in vocab.token_freqs]
d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',
         xscale='log', yscale='log')
```


![](https://zjpicture.oss-cn-beijing.aliyuncs.com/img/20211206224653.svg)
    


* 通过此图我们可以发现某些根本的东西：词频以一种明确的方式迅速衰减。
    * 将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。
    * 这意味着单词的频率满足 *齐普夫定律*（Zipf's law），即第 $i$ 个最常用单词的频率 $n_i$ 为：
        $$n_i \propto \frac{1}{i^\alpha},$$
        等价于
        $$\log n_i = -\alpha \log i + c,$$
    * 其中 $\alpha$ 是刻画分布的指数，$c$ 是常数。
* 这就告诉我们想要通过计数统计和平滑来建模单词是不可行的，这样建模的结果会大大*高估尾部单词的频率，也就是所谓的不常用单词*。那么[**其他的词元组合，比如二元语法、三元语法等等，又会如何呢？**]让我们看看二元语法的频率是否与一元语法的频率表现出相同的行为方式。


```python
bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]
bigram_vocab = d2l.Vocab(bigram_tokens)
bigram_vocab.token_freqs[:10]
```




    [(('of', 'the'), 309),
     (('in', 'the'), 169),
     (('i', 'had'), 130),
     (('i', 'was'), 112),
     (('and', 'the'), 109),
     (('the', 'time'), 102),
     (('it', 'was'), 99),
     (('to', 'the'), 85),
     (('as', 'i'), 78),
     (('of', 'a'), 73)]



* 这里值得注意：在十个最频繁的词对中，有九个是由两个停用词组成的，只有一个与“the time”有关。进一步，让我们看看三元语法的频率是否表现出相同的行为方式。


```python
trigram_tokens = [triple for triple in zip(
    corpus[:-2], corpus[1:-1], corpus[2:])]
trigram_vocab = d2l.Vocab(trigram_tokens)
trigram_vocab.token_freqs[:10]
```




    [(('the', 'time', 'traveller'), 59),
     (('the', 'time', 'machine'), 30),
     (('the', 'medical', 'man'), 24),
     (('it', 'seemed', 'to'), 16),
     (('it', 'was', 'a'), 15),
     (('here', 'and', 'there'), 15),
     (('seemed', 'to', 'me'), 14),
     (('i', 'did', 'not'), 14),
     (('i', 'saw', 'the'), 13),
     (('i', 'began', 'to'), 13)]



* 最后，让我们[**直观地对比三种模型中的词元频率**]：一元语法、二元语法和三元语法。


```python
bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]
trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]
d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',
         ylabel='frequency: n(x)', xscale='log', yscale='log',
         legend=['unigram', 'bigram', 'trigram'])
```


![](https://zjpicture.oss-cn-beijing.aliyuncs.com/img/20211206224709.svg)
    


* 首先，除了一元语法词，单词序列似乎也遵循齐普夫定律，尽管公式中的指数 $\alpha$ 更小（指数的大小受序列长度的影响）。
* 其次，词汇表中 $n$ 元组的数量并没有那么大，这说明语言中存在相当多的结构，这些结构给了我们应用模型的希望。
* 第三，很多 $n$ 元组很少出现，这使得拉普拉斯平滑非常不适合语言建模。作为代替，我们将使用基于深度学习的模型。

## 读取长序列数据

* 由于序列数据本质上是连续的，因此我们在处理数据时需要解决这个问题。在序列模型中我们以一种相当特别的方式做到了这一点。
* 当序列变得太长而不能被模型一次性全部处理时，我们可能希望拆分这样的序列方便模型读取。
* 假设我们将使用神经网络来训练语言模型，模型中的网络一次处理具有预定义长度（例如 $n$ 个时间步）的一个小批量序列。现在的问题是如何[**随机地生成一个小批量数据的特征和标签以供读取。**]
    * 首先，由于文本序列可以是任意长的，例如整本《时光机器》(*The Time Machine*)，于是任意长的序列可以被我们划分为具有相同时间步数的子序列。当训练我们的神经网络时，这样的小批量子序列将被输入到模型中。假设网络一次只处理具有 $n$ 个时间步的子序列。 下图画出了从原始文本序列获得子序列的所有不同的方式，其中 $n=5$，并且每个时间步的词元对应于一个字符。请注意，因为我们可以选择任意偏移量来指示初始位置，所以我们有相当大的自由度。
    ![BTducZ](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/BTducZ.png)
* 我们可以从随机偏移量开始划分序列，以同时获得 *覆盖性*（coverage）和 *随机性*（randomness）。下面，我们将实现 *随机采样*（random sampling）和 *顺序分区*（sequential partitioning）策略。

### 随机采样

* (**在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列。**)在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。
* 对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元，因此*标签是移位了一个词元的原始序列*。
* 下面的代码每次都从数据中随机生成一个小批量。在这里，参数 `batch_size` 指定了每个小批量中子序列样本的数目，参数 `num_steps` 是每个子序列中预定义的时间步数。


```python
def seq_data_iter_random(corpus, batch_size, num_steps):  #@save
    """使用随机抽样生成一个小批量子序列。"""
    # 从随机偏移量开始对序列进行分区，随机范围包括`num_steps - 1`
    corpus = corpus[random.randint(0, num_steps - 1):]
    # 减去1，是因为我们需要考虑标签
    num_subseqs = (len(corpus) - 1) // num_steps
    # 长度为`num_steps`的子序列的起始索引
    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
    # 在随机抽样的迭代过程中，
    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻
    random.shuffle(initial_indices)

    def data(pos):
        # 返回从`pos`位置开始的长度为`num_steps`的序列
        return corpus[pos: pos + num_steps]

    num_batches = num_subseqs // batch_size
    for i in range(0, batch_size * num_batches, batch_size):
        # 在这里，`initial_indices`包含子序列的随机起始索引
        initial_indices_per_batch = initial_indices[i: i + batch_size]
        X = [data(j) for j in initial_indices_per_batch]
        Y = [data(j + 1) for j in initial_indices_per_batch]
        yield tf.constant(X), tf.constant(Y)
```

* 让我们[**生成一个从 $0$ 到 $34$ 的序列**]。假设批量大小为 $2$ ，时间步数为 $5$，这意味着可以生成 $\lfloor (35 - 1) / 5 \rfloor= 6$ 个“特征－标签”子序列对。设置小批量大小为 $2$ 时，我们只能得到 $3$ 个小批量。


```python
my_seq = list(range(35))
for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):
    print('X: ', X, '\nY:', Y)
```

    X:  tf.Tensor(
    [[26 27 28 29 30]
     [ 1  2  3  4  5]], shape=(2, 5), dtype=int32) 
    Y: tf.Tensor(
    [[27 28 29 30 31]
     [ 2  3  4  5  6]], shape=(2, 5), dtype=int32)
    X:  tf.Tensor(
    [[11 12 13 14 15]
     [ 6  7  8  9 10]], shape=(2, 5), dtype=int32) 
    Y: tf.Tensor(
    [[12 13 14 15 16]
     [ 7  8  9 10 11]], shape=(2, 5), dtype=int32)
    X:  tf.Tensor(
    [[16 17 18 19 20]
     [21 22 23 24 25]], shape=(2, 5), dtype=int32) 
    Y: tf.Tensor(
    [[17 18 19 20 21]
     [22 23 24 25 26]], shape=(2, 5), dtype=int32)


### 顺序分区

* 在迭代过程中，除了对原始序列可以随机抽样外，我们还可以[**保证两个相邻的小批量中的子序列在原始序列上也是相邻的**]。这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。


```python
def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save
    """使用顺序分区生成一个小批量子序列。"""
    # 从随机偏移量开始划分序列
    offset = random.randint(0, num_steps)
    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size
    Xs = tf.constant(corpus[offset: offset + num_tokens])
    Ys = tf.constant(corpus[offset + 1: offset + 1 + num_tokens])
    Xs = tf.reshape(Xs, (batch_size, -1))
    Ys = tf.reshape(Ys, (batch_size, -1))
    num_batches = Xs.shape[1] // num_steps
    for i in range(0, num_batches * num_steps, num_steps):
        X = Xs[:, i: i + num_steps]
        Y = Ys[:, i: i + num_steps]
        yield X, Y
```

* 基于相同的设置，通过顺序分区[**读取每个小批量的子序列的特征 `X` 和标签 `Y`**]。通过将它们打印出来可以注意到，迭代期间来自两个相邻的小批量中的子序列在原始序列中确实是相邻的。


```python
for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):
    print('X: ', X, '\nY:', Y)
```

    X:  tf.Tensor(
    [[ 2  3  4  5  6]
     [18 19 20 21 22]], shape=(2, 5), dtype=int32) 
    Y: tf.Tensor(
    [[ 3  4  5  6  7]
     [19 20 21 22 23]], shape=(2, 5), dtype=int32)
    X:  tf.Tensor(
    [[ 7  8  9 10 11]
     [23 24 25 26 27]], shape=(2, 5), dtype=int32) 
    Y: tf.Tensor(
    [[ 8  9 10 11 12]
     [24 25 26 27 28]], shape=(2, 5), dtype=int32)
    X:  tf.Tensor(
    [[12 13 14 15 16]
     [28 29 30 31 32]], shape=(2, 5), dtype=int32) 
    Y: tf.Tensor(
    [[13 14 15 16 17]
     [29 30 31 32 33]], shape=(2, 5), dtype=int32)


* 现在，我们[**将上面的两个采样函数包装到一个类中**]，以便稍后可以将其用作数据迭代器。


```python
class SeqDataLoader:  #@save
    """加载序列数据的迭代器。"""
    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
        if use_random_iter:
            self.data_iter_fn = d2l.seq_data_iter_random
        else:
            self.data_iter_fn = d2l.seq_data_iter_sequential
        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)
        self.batch_size, self.num_steps = batch_size, num_steps

    def __iter__(self):
        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)
```

* 最后，我们定义了一个函数 `load_data_time_machine` ，它同时返回数据迭代器和词汇表，因此可以与我们定义的其他带有 `load_data` 前缀的函数类似地使用。


```python
def load_data_time_machine(batch_size, num_steps,  #@save
                           use_random_iter=False, max_tokens=10000):
    """返回时光机器数据集的迭代器和词汇表。"""
    data_iter = SeqDataLoader(
        batch_size, num_steps, use_random_iter, max_tokens)
    return data_iter, data_iter.vocab
```
# 循环神经网络

* 在$n$ 元语法模型中，单词 $x_t$ 在时间步 $t$ 的条件概率仅取决于前面 $n-1$ 个单词。如果我们想将时间步 $t-(n-1)$ 之前的单词的可能产生的影响合并到 $x_t$ 上就需要增加 $n$，然而模型参数的数量也会随之呈指数增长，因为词表 $\mathcal{V}$ 需要存储 $|\mathcal{V}|^n$ 个数字，因此与其将 $P(x_t \mid x_{t-1}, \ldots, x_{t-n+1})$ 模型化，不如使用隐变量模型：

    $$P(x_t \mid x_{t-1}, \ldots, x_1) \approx P(x_t \mid h_{t-1}),$$
    * 其中 $h_{t-1}$ 是 *隐藏状态*（也称为隐藏变量），其存储了到时间步 $t-1$ 的序列信息。通常，可以基于当前输入 $x_{t}$ 和先前隐藏状态 $h_{t-1}$ 来计算时间步 $t$ 处的任何时间的隐藏状态：
    $$h_t = f(x_{t}, h_{t-1}).$$
    * 对于一个足够强大的函数 $f$，隐变量模型不是近似值。毕竟 $h_t$ 是可以仅仅存储到目前为止观察到的所有数据，然而这样的操作可能会使计算和存储的代价都变得昂贵。
* 如上所述，隐藏层是在输入到输出的路径上以观测角度来理解的隐藏的层，而隐藏状态则是在给定步骤所做的任何事情以技术角度来定义的 *输入*，并且这些状态只能通过先前时间步的数据来计算。
* *循环神经网络*（Recurrent neural networks， RNNs）是具有隐藏状态的神经网络。

## 无隐藏状态的神经网络

* 让我们来看一看只有单隐藏层的多层感知机。设隐藏层的激活函数为 $\phi$。给定一个小批量样本 $\mathbf{X} \in \mathbb{R}^{n \times d}$，其中批量大小为 $n$，输入维度为 $d$，则隐藏层的输出 $\mathbf{H} \in \mathbb{R}^{n \times h}$ 通过下式计算：
    $$\mathbf{H} = \phi(\mathbf{X} \mathbf{W}_{xh} + \mathbf{b}_h).$$
    * 在上式中，我们拥有的隐藏层权重参数为  $\mathbf{W}_{xh} \in \mathbb{R}^{d \times h}$、偏置参数为 $\mathbf{b}_h \in \mathbb{R}^{1 \times h}$，以及隐藏单元的数目为 $h$。因此求和时将应用广播机制。
* 接下来，将隐藏变量 $\mathbf{H}$ 用作输出层的输入。输出层由下式给出：
    $$\mathbf{O} = \mathbf{H} \mathbf{W}_{hq} + \mathbf{b}_q,$$
    * 其中，$\mathbf{O} \in \mathbb{R}^{n \times q}$ 是输出变量，$\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}$ 是权重参数，$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$ 是输出层的偏置参数。如果是分类问题，我们可以用 $\text{softmax}(\mathbf{O})$ 来计算输出类别的概率分布。
* 这完全类似于中一般的回归问题，只要可以随机选择“特征-标签”对，并且通过自动微分和随机梯度下降能够学习网络参数就可以了。

## 有隐藏状态的循环神经网络

* 当我们有隐藏状态后，情况就完全不同了。让我们更详细地看看这个结构：
    * 假设我们在时间步$t$有小批量输入$\mathbf{X}_t \in \mathbb{R}^{n \times d}$。换言之，对于$n$个序列样本的小批量，$\mathbf{X}_t$的每一行对应于来自该序列的时间步$t$处的一个样本。
    * 接下来，用$\mathbf{H}_t  \in \mathbb{R}^{n \times h}$表示时间步$t$的隐藏变量。与多层感知机不同的是，我们在这里保存了前一个时间步的隐藏变量$\mathbf{H}_{t-1}$，并引入了一个新的权重参数$\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$来描述如何在当前时间步中使用前一个时间步的隐藏变量。具体地说，当前时间步隐藏变量的计算由当前时间步的输入与前一个时间步的隐藏变量一起确定：
    $$\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}  + \mathbf{b}_h).$$
* 与没有隐藏状态相比， 上式中多添加了一项$\mathbf{H}_{t-1} \mathbf{W}_{hh}$，从而实例化了$h_t = f(x_{t}, h_{t-1}).$。从相邻时间步的隐藏变量$\mathbf{H}_t$和$\mathbf{H}_{t-1}$之间的关系可知，这些变量捕获并保留了序列直到其当前时间步的历史信息，就如当前时间步下神经网络的状态或记忆，因此这样的隐藏变量被称为 *隐藏状态*（hidden state）。
* 由于在当前时间步中隐藏状态使用的定义与前一个时间步中使用的定义相同，因此上式的计算是 *循环的*（recurrent）。于是基于循环计算的隐状态神经网络被命名为 *循环神经网络*（recurrent neural networks）。在循环神经网络中执行上式计算的层称为 *循环层*（recurrent layers）。
* 有许多不同的方法可以构建循环神经网络，由上式定义的隐藏状态的循环神经网络是非常常见的一种。对于时间步$t$，输出层的输出类似于多层感知机中的计算：
    $$\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q.$$
    * 循环神经网络的参数包括隐藏层的权重$\mathbf{W}_{xh} \in \mathbb{R}^{d \times h}, \mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$和偏置$\mathbf{b}_h \in \mathbb{R}^{1 \times h}$，以及输出层的权重$\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}$和偏置$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$。
    * 即使在不同的时间步，循环神经网络也总是使用这些模型参数。因此，循环神经网络的参数开销不会随着时间步的增加而增加。
* 下图展示了循环神经网络在三个相邻时间步的计算逻辑。在任意时间步$t$，隐藏状态的计算可以被视为：
    1. 拼接当前时间步$t$的输入$\mathbf{X}_t$和前一时间步$t-1$的隐藏状态$\mathbf{H}_{t-1}$；
    2. 将拼接的结果送入带有激活函数$\phi$的全连接层。全连接层的输出是当前时间步$t$的隐藏状态$\mathbf{H}_t$。
* 在本例中，模型参数是$\mathbf{W}_{xh}$和$\mathbf{W}_{hh}$的拼接，以及$\mathbf{b}_h$的偏置。当前时间步$t$的隐藏状态$\mathbf{H}_t$将参与计算下一时间步$t+1$的隐藏状态$\mathbf{H}_{t+1}$。而且$\mathbf{H}_t$还将送入全连接输出层用于计算当前时间步$t$的输出$\mathbf{O}_t$。
    ![jRxegq](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/jRxegq.png)
* 我们刚才提到，隐藏状态中$\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}$的计算，相当于$\mathbf{X}_t$和$\mathbf{H}_{t-1}$的拼接与$\mathbf{W}_{xh}$和$\mathbf{W}_{hh}$的拼接的矩阵乘法。
* 下面我们使用一个简单的代码片段来说明。首先，我们定义矩阵`X`、`W_xh`、`H`和`W_hh`，它们的形状分别为$(3，1)$、$(1，4)$、$(3，4)$和$(4，4)$。分别将`X`乘以`W_xh`，将`H`乘以`W_hh`，然后将这两个乘法相加，我们得到一个形状为$(3，4)$的矩阵。


```python
import tensorflow as tf
from d2l import tensorflow as d2l
```


```python
X, W_xh = tf.random.normal((3, 1), 0, 1), tf.random.normal((1, 4), 0, 1)
H, W_hh = tf.random.normal((3, 4), 0, 1), tf.random.normal((4, 4), 0, 1)
test1 = tf.matmul(X, W_xh) + tf.matmul(H, W_hh)
test1.shape
```




    TensorShape([3, 4])



* 现在，我们沿列（轴1）拼接矩阵`X`和`H`，沿行（轴0）拼接矩阵`W_xh`和`W_hh`。这两个拼接分别产生形状$(3, 5)$和形状$(5, 4)$的矩阵。再将这两个拼接的矩阵相乘，我们得到与上面相同形状$(3, 4)$的输出矩阵。


```python
test2 = tf.matmul(tf.concat((X, H), 1), tf.concat((W_xh, W_hh), 0)) 
test2.shape
```




    TensorShape([3, 4])



## 基于循环神经网络的字符级语言模型

* 在之前提到的语言模型中，我们的目标是根据过去的和当前的词元预测下一个词元，因此我们将原始序列移位一个词元作为标签。
* 接下来，我们使用循环神经网络来构建语言模型。设小批量大小为1，批量中的那个文本序列为“machine”。为了简化后续部分的训练，我们考虑使用 *字符级语言模型*（character-level language model），将文本词元化为字符而不是单词。 下图演示了如何通过基于字符级语言建模的循环神经网络使用当前的和先前的字符预测下一个字符：
    ![aNmuxD](https://gitee.com/zhangjie0524/picgo/raw/master/uPic/aNmuxD.png)
* 在训练过程中，我们对每个时间步的输出层的输出进行softmax操作，然后利用交叉熵损失计算模型输出和标签之间的误差。由于隐藏层中隐藏状态的循环计算，上图中的第$3$个时间步的输出$\mathbf{O}_3$由文本序列“m”、“a”和“c”确定。由于训练数据中这个文本序列的下一个字符是“h”，因此第$3$个时间步的损失将取决于下一个字符的概率分布，而下一个字符是基于特征序列“m”、“a”、“c”和这个时间步的标签“h”生成的。
* 在实践中，我们使用的批量大小为$n>1$，每个词元都由一个$d$维向量表示。因此，在时间步$t$输入$\mathbf X_t$将是一个$n\times d$矩阵。

## 困惑度（Perplexity）

* 我们可以通过一个序列中所有的$n$个词元的交叉熵损失的平均值来衡量语言模型的质量：
    $$\frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1),$$
* 其中$P$由语言模型给出，$x_t$是在时间步$t$从该序列中观察到的实际词元。这使得不同长度的文档的性能具有了可比性。由于历史原因，自然语言处理的科学家更喜欢使用一个叫做 *困惑度*（perplexity）的量。简而言之，它是上面式子的指数：
    $$\exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right).$$
* 当我们决定下一个词元是哪个时，困惑度的最好的理解可以是*下一个词元的实际选择数的调和平均数*。让我们看看一些案例：
    * 在最好的情况下，模型总是完美地估计标签词元的概率为1。在这种情况下，模型的困惑度为1。
    * 在最坏的情况下，模型总是预测标签词元的概率为0。在这种情况下，困惑度是正无穷大。
    * 在基线上，该模型的预测是词汇表的所有可用词元上的均匀分布。在这种情况下，困惑度等于词汇表中唯一词元的数量。
* 我们基于循环神经网络实现字符级语言模型，将使用困惑度来评估。

# 循环神经网络的从零开始实现
* 下面从头开始基于循环神经网络实现字符级语言模型。这个模型将在前面构建的《时光机器》一书的数据集上训练。
* 我们先读取数据集。注：[读取数据集函数相关定义](https://www.zestaken.top/post/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0)


```python
%matplotlib inline
import math
import numpy as np
import tensorflow as tf
from d2l import tensorflow as d2l
```


```python
batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
```


```python
train_random_iter, vocab_random_iter = d2l.load_data_time_machine(
    batch_size, num_steps, use_random_iter=True)
```

## 独热编码

* 在 `train_iter` 中，每个词元都表示为一个数字索引。将这些索引直接输入神经网络可能会使学习变得困难。我们通常将每个词元表示为更具表现力的特征向量。最简单的表示称为*独热编码*（one-hot encoding）
* 简言之，将每个索引映射为相互不同的单位向量：假设词汇表中不同词元的数目为$N$（即 `len(vocab)`），词元索引的范围为$0$到$N-1$。如果词元的索引是整数$i$，那么我们创建一个长度为$N$的全$0$向量，并将第$i$处的元素设置为$1$。此向量是原始词元的一个独热向量。索引为$0$和$2$的独热向量如下所示。


```python
tf.one_hot(tf.constant([0, 2]), len(vocab))
```




    <tf.Tensor: shape=(2, 28), dtype=float32, numpy=
    array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>



* 我们每次采样的(**小批量数据形状是（批量大小, 时间步数）。**) `one_hot`函数将这样一个小批量数据转换成三维张量，张量的最后一个维度等于词汇表大小（`len(vocab)`）。我们经常转换输入的维度，以便获得形状为（时间步数, 批量大小, 词汇表大小）的输出。这将使我们能够更方便地通过最外层的维度，一步一步地更新小批量数据的隐藏状态。


```python
X = tf.reshape(tf.range(10), (2, 5))
tf.one_hot(tf.transpose(X), 28).shape
```




    TensorShape([5, 2, 28])



## 初始化模型参数

* 接下来，我们[**初始化循环神经网络模型的模型参数**]。隐藏单元数`num_hiddens`是一个可调的超参数。当训练语言模型时，输入和输出来自相同的词汇表。因此，它们具有相同的维度，即词汇表的大小。


```python
def get_params(vocab_size, num_hiddens):
    num_inputs = num_outputs = vocab_size
    
    def normal(shape):
        return tf.random.normal(shape=shape,stddev=0.01,mean=0,dtype=tf.float32)

    # 隐藏层参数
    W_xh = tf.Variable(normal((num_inputs, num_hiddens)), dtype=tf.float32)
    W_hh = tf.Variable(normal((num_hiddens, num_hiddens)), dtype=tf.float32)
    b_h = tf.Variable(tf.zeros(num_hiddens), dtype=tf.float32) # 偏置初始化为0
    # 输出层参数
    W_hq = tf.Variable(normal((num_hiddens, num_outputs)), dtype=tf.float32)
    b_q = tf.Variable(tf.zeros(num_outputs), dtype=tf.float32)
    params = [W_xh, W_hh, b_h, W_hq, b_q]
    return params
```

## 循环神经网络模型

* 为了定义循环神经网络模型，我们首先需要[**一个`init_rnn_state`函数在初始化时返回隐藏状态**]。函数的返回是一个张量，张量全用0填充，形状为（批量大小, 隐藏单元数）。可能会遇到隐藏状态包含多个变量的情况，使用元组可以处理得更容易。


```python
def init_rnn_state(batch_size, num_hiddens):
    return (tf.zeros((batch_size, num_hiddens)), )
```

* [**下面的`rnn`函数定义了如何在一个时间步内计算隐藏状态和输出。**]
* 请注意，循环神经网络模型通过`inputs`最外层的维度实现循环，以便逐时间步更新小批量数据的隐藏状态`H`。此外，这里使用$\tanh$函数作为激活函数。当元素在实数上满足均匀分布时，$\tanh$函数的平均值为0。


```python
def rnn(inputs, state, params):
    # `inputs`的形状：(`时间步数量`，`批量大小`，`词表大小`)
    W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    # `X`的形状：(`批量大小`，`词表大小`)
    for X in inputs:
        X = tf.reshape(X,[-1,W_xh.shape[0]])
        H = tf.tanh(tf.matmul(X, W_xh) + tf.matmul(H, W_hh) + b_h)
        Y = tf.matmul(H, W_hq) + b_q
        outputs.append(Y)
    return tf.concat(outputs, axis=0), (H,)
```

* 定义了所有需要的函数之后，接下来我们[**创建一个类来包装这些函数**]，并存储从零开始实现的循环神经网络模型的参数。


```python
class RNNModelScratch: #@save
    """从零开始实现的循环神经网络模型"""
    def __init__(self, vocab_size, num_hiddens,
                 init_state, forward_fn):
        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens
        self.init_state, self.forward_fn = init_state, forward_fn

    def __call__(self, X, state, params):
        X = tf.one_hot(tf.transpose(X), self.vocab_size)
        X = tf.cast(X, tf.float32)
        return self.forward_fn(X, state, params)

    def begin_state(self, batch_size):
        return self.init_state(batch_size, self.num_hiddens)
```

* 让我们[**检查输出是否具有正确的形状**]，例如，是否保证了隐藏状态的维数保持不变。


```python
# 定义tensorflow训练策略
device_name = d2l.try_gpu()._device_name # 尝试用GPU训练
strategy = tf.distribute.OneDeviceStrategy(device_name)

num_hiddens = 512
with strategy.scope():
    net = RNNModelScratch(len(vocab), num_hiddens, init_rnn_state, rnn)
state = net.begin_state(X.shape[0])
params = get_params(len(vocab), num_hiddens)
Y, new_state = net(X, state, params)
Y.shape, len(new_state), new_state[0].shape
```




    (TensorShape([10, 28]), 1, TensorShape([2, 512]))



* 我们可以看到输出形状是（时间步数$\times$批量大小，词汇表大小），而隐藏状态形状保持不变，即（批量大小, 隐藏单元数）。

## 预测

* 让我们[**首先定义预测函数来生成`prefix`之后的新字符**]，其中的`prefix`是一个用户提供的包含多个字符的字符串。
* 预热期：在循环遍历`prefix`中的开始字符时，我们不断地将隐藏状态传递到下一个时间步，但是不生成任何输出。这被称为“预热”（warm-up）期，因为在此期间模型会自我更新（例如，更新隐藏状态），但不会进行预测。
* 预热期结束后，隐藏状态的值通常比刚开始的初始值更适合预测，从而预测字符并输出它们。


```python
def predict_ch8(prefix, num_preds, net, vocab, params):  #@save
    """在`prefix`后面生成新字符。"""
    state = net.begin_state(batch_size=1)
    outputs = [vocab[prefix[0]]]
    get_input = lambda: tf.reshape(tf.constant([outputs[-1]]), (1, 1)).numpy()
    for y in prefix[1:]:  # 预热期
        _, state = net(get_input(), state, params)
        outputs.append(vocab[y])
    for _ in range(num_preds):  # 预测`num_preds`步
        y, state = net(get_input(), state, params)
        outputs.append(int(y.numpy().argmax(axis=1).reshape(1)))
    return ''.join([vocab.idx_to_token[i] for i in outputs])
```

* 现在我们可以测试`predict_ch8`函数。我们将前缀指定为`time traveller ` ，并基于这个前缀生成10个后续字符。鉴于我们还没有训练网络，它会生成荒谬的预测结果。


```python
predict_ch8('time traveller ', 10, net, vocab, params)
```




    'time traveller bdkcerfkce'



## 梯度裁剪

* 对于长度为$T$的序列，我们在迭代中计算这$T$个时间步上的梯度，将会在反向传播过程中产生长度为$\mathcal{O}(T)$的矩阵乘法链。
* 当$T$较大时，它可能导致数值不稳定，例如可能导致梯度爆炸或梯度消失。因此，循环神经网络模型往往需要额外的支持来稳定训练。
* 一般来说，当解决优化问题时，我们对模型参数采用更新步骤，假定在向量形式的$\mathbf{x}$中，或者在小批量数据的负梯度$\mathbf{g}$方向上。
    * 例如，使用$\eta > 0$作为学习率时，在一次迭代中，我们将$\mathbf{x}$更新为$\mathbf{x} - \eta \mathbf{g}$。让我们进一步假设目标函数$f$表现良好，表示伴随常数 $L$ 的 *利普希茨连续*（Lipschitz continuous）。也就是说，对于任意$\mathbf{x}$和$\mathbf{y}$我们有：
    $$|f(\mathbf{x}) - f(\mathbf{y})| \leq L \|\mathbf{x} - \mathbf{y}\|.$$
* 在这种情况下，我们可以安全地假设，如果我们通过$\eta \mathbf{g}$更新参数向量，那么：
    $$|f(\mathbf{x}) - f(\mathbf{x} - \eta\mathbf{g})| \leq L \eta\|\mathbf{g}\|,$$
    * 这意味着我们不会观察到超过$L \eta \|\mathbf{g}\|$的变化。这既是坏事也是好事。在坏的一面，它限制了取得进展的速度；而在好的一面，它限制了事情变糟的程度，当我们朝着错误的方向前进时。
* 有时梯度可能很大，从而优化算法可能无法收敛。我们可以通过降低$\eta$的学习率来解决这个问题。
* **梯度剪裁**：当我们很少得到很大的梯度时，通过将梯度$\mathbf{g}$投影回给定半径（例如$\theta$）的球来裁剪梯度$\mathbf{g}$。如下式：
    $$\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{\|\mathbf{g}\|}\right) \mathbf{g}.$$
    * 通过这样做，我们知道梯度范数永远不会超过$\theta$，并且更新后的梯度完全与$\mathbf{g}$的原始方向对齐。
    * 它还有一个值得拥有的副作用，即限制任何给定的小批量数据（以及其中任何给定的样本）对参数向量的影响，这赋予了模型一定程度的稳定性。
    * 梯度裁剪提供了一个快速修复梯度爆炸的方法。
* 下面我们定义一个函数来裁剪模型的梯度，模型是从零开始实现的模型或由高级API构建的模型。另外，我们计算了所有模型参数的梯度的范数。


```python
def grad_clipping(grads, theta): #@save
    """裁剪梯度。"""
    theta = tf.constant(theta, dtype=tf.float32)
    norm = tf.math.sqrt(sum((tf.reduce_sum(grad ** 2)).numpy()
                        for grad in grads))
    norm = tf.cast(norm, tf.float32)
    new_grad = []
    if tf.greater(norm, theta):
        for grad in grads:
            new_grad.append(grad * theta / norm)
    else:
        for grad in grads:
            new_grad.append(grad)
    return new_grad
```

## 训练

* 在训练模型之前，让我们[**定义一个函数在一个迭代周期内训练模型**]。它与我们训练线性模型的方式有三个不同之处：
    1. 序列数据的不同采样方法（随机采样和顺序分区）将导致隐藏状态初始化的差异。
    2. 我们在更新模型参数之前裁剪梯度。这样的操作即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。
    3. 我们用困惑度来评价模型。这样的度量确保了不同长度的序列具有可比性。
* 当使用顺序分区时，我们只在每个迭代周期的开始位置初始化隐藏状态。
    * 由于下一个小批量数据中的第$i$个子序列样本与当前第$i$个子序列样本相邻，因此当前小批量数据最后一个样本的隐藏状态，将用于初始化下一个小批量数据第一个样本的隐藏状态。
    * 这样，存储在隐藏状态中的序列的历史信息可以在一个迭代周期内流经相邻的子序列。
    * 然而，在任何一点隐藏状态的计算，都依赖于同一迭代周期中前面所有的小批量数据，这使得梯度计算变得复杂。
    * 为了降低计算量，我们在处理任何一个小批量数据之前先*分离梯度*，使得隐藏状态的梯度计算总是限制在一个小批量数据的时间步内。
* 当使用随机抽样时，因为每个样本都是在一个随机位置抽样的，因此需要为每个迭代周期重新初始化隐藏状态。它既可以是从头开始实现的sgd优化函数，也可以是深度学习框架中内置的优化函数。


```python
#@save
def train_epoch_ch8(net, train_iter, loss, updater, params, use_random_iter):
    """训练模型一个迭代周期"""
    state, timer = None, d2l.Timer()
    metric = d2l.Accumulator(2)  # 训练损失之和, 词元数量
    for X, Y in train_iter:
        if state is None or use_random_iter:
            # 在第一次迭代或使用随机抽样时初始化`state`
            state = net.begin_state(batch_size=X.shape[0])
        with tf.GradientTape(persistent=True) as g:
            g.watch(params)
            y_hat, state= net(X, state, params)
            y = tf.reshape(tf.transpose(Y), (-1))
            l = loss(y, y_hat)
        grads = g.gradient(l, params)
        grads = grad_clipping(grads, 1)
        updater.apply_gradients(zip(grads, params))
        
        # Keras默认返回一个批量中的平均损失
        # l_sum = l * float(d2l.size(y)) if isinstance(
        #     loss, tf.keras.losses.Loss) else tf.reduce_sum(l)
        metric.add(l * d2l.size(y), d2l.size(y))
    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()
```

* [**循环神经网络模型的训练函数既支持从零开始实现，也可以使用高级API来实现。**]


```python
#@save
def train_ch8(net, train_iter, vocab, num_hiddens, lr, num_epochs, strategy,
              use_random_iter=False):
    """训练模型。"""
    with strategy.scope():
        params = get_params(len(vocab), num_hiddens)
        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
        updater = tf.keras.optimizers.SGD(lr)
    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',
                            legend=['train'], xlim=[10, num_epochs])
    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, params)
    # 训练和预测
    for epoch in range(num_epochs):
        ppl, speed = train_epoch_ch8(
             net, train_iter, loss, updater, params, use_random_iter)
        if (epoch + 1) % 10 == 0:
            print(predict('time traveller'))
            animator.add(epoch + 1, [ppl])
    device = d2l.try_gpu()._device_name
    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')
    print(predict('time traveller'))
    print(predict('traveller'))
```

* [**现在，我们训练循环神经网络模型。**]
* 因为我们在数据集中只使用10000个词元，所以模型需要*更多的迭代周期*来更好地收敛。


```python
num_epochs, lr = 500, 1
train_ch8(net, train_iter, vocab, num_hiddens, lr, num_epochs, strategy)
```

    困惑度 1.2, 14664.5 词元/秒 /GPU:0
    time travelleryou can show black is whit ibycinas on a gothat ll
    travelleryou can show black is whit firmerence try the fors




![](https://zjpicture.oss-cn-beijing.aliyuncs.com/img/20211206224832.svg)
    


* [**最后，让我们检查一下使用随机抽样方法的结果。**]


```python
params = get_params(len(vocab_random_iter), num_hiddens)
train_ch8(net, train_random_iter, vocab_random_iter, num_hiddens, lr,
          num_epochs, strategy, use_random_iter=True)
```

    困惑度 1.4, 15580.3 词元/秒 /GPU:0
    time traveller with a slight accession ofcheerfulness really thi
    traveller came back andfilby s anecdote collapsedthe thing 




![](https://zjpicture.oss-cn-beijing.aliyuncs.com/img/20211206224848.svg)
    
# 循环神经网络的简洁实现

* 下面将使用深度学习框架的高级API提供的函数更有效地实现相同的语言模型。
* 从读取时光机器数据集开始。


```python
import tensorflow as tf
from d2l import tensorflow as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
```

## 定义模型

* 高级API提供了循环神经网络的实现。
* 我们构造一个具有256个隐藏单元的单隐藏层的循环神经网络层`rnn_layer`。


```python
num_hiddens = 256
rnn_cell = tf.keras.layers.SimpleRNNCell(num_hiddens,
    kernel_initializer='glorot_uniform')
rnn_layer = tf.keras.layers.RNN(rnn_cell, time_major=True,
    return_sequences=True, return_state=True)
```


```python
state = rnn_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)
state.shape
```




    TensorShape([32, 256])



* [**通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。**]
* 需要强调的是，`rnn_layer`的“输出”（`Y`）不涉及输出层的计算：它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。


```python
X = tf.random.uniform((num_steps, batch_size, len(vocab)))
Y, state_new = rnn_layer(X, state)
Y.shape, len(state_new), state_new[0].shape
```




    (TensorShape([35, 32, 256]), 32, TensorShape([256]))



* 我们为一个完整的循环神经网络模型定义了一个`RNNModel`类。
* 注意，`rnn_layer`只包含隐藏的循环层，我们还需要创建一个单独的输出层。


```python
#@save
class RNNModel(tf.keras.layers.Layer):
    def __init__(self, rnn_layer, vocab_size, **kwargs):
        super(RNNModel, self).__init__(**kwargs)
        self.rnn = rnn_layer
        self.vocab_size = vocab_size
        self.dense = tf.keras.layers.Dense(vocab_size)

    def call(self, inputs, state):
        X = tf.one_hot(tf.transpose(inputs), self.vocab_size)
        # rnn返回两个以上的值
        Y, *state = self.rnn(X, state)
        output = self.dense(tf.reshape(Y, (-1, Y.shape[-1])))
        return output, state

    def begin_state(self, *args, **kwargs):
        return self.rnn.cell.get_initial_state(*args, **kwargs)
```

## 训练与预测

* 在训练模型之前，让我们[**基于一个具有随机权重的模型进行预测**]。


```python
device_name = d2l.try_gpu()._device_name
strategy = tf.distribute.OneDeviceStrategy(device_name)
with strategy.scope():
    net = RNNModel(rnn_layer, vocab_size=len(vocab))

d2l.predict_ch8('time traveller', 10, net, vocab)
```




    'time travelleryscczvsuys'



* 这种模型根本不能输出好的结果。接下来，我们使用从零实现中定义的超参数调用`train_ch8`，并且[**使用高级API训练模型**]。


```python
num_epochs, lr = 500, 1
d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, strategy)
```

    perplexity 1.3, 23537.9 tokens/sec on /GPU:0
    time travellerif singanyer at se moverad llistovit fertong thaco
    travelleryout coredithtenctour me the four dimensions ald t




![](https://zjpicture.oss-cn-beijing.aliyuncs.com/img/20211206224927.svg)
    


* 与从零实现相比，由于深度学习框架的高级API对代码进行了更多的优化，该模型在较短的时间内达到了较低的困惑度。

# 注

[参考教程](https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/index.html)





